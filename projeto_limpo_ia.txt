
============================================================
ARQUIVO: settings.yaml
LOCAL: settings.yaml
============================================================

# ==============================================================================
# üåæ Agro Risk Engine - Data Pipeline CONFIGURATION (UNIFIED)
# ==============================================================================
project_name: "Agro Risk Engine - Data Pipeline"
version: "2.6.0"

# --- 1. Market Data (Ativos) ---
tickers:
  - "ZS=F"        # Soja Chicago (Contrato Atual/Pr√≥ximo)
  - "ZC=F"        # Milho Chicago
  - "ZW=F"        # Trigo Chicago
  - "KE=F"        # Trigo Kansas
  - "USDBRL=X"    # D√≥lar
  - "CL=F"        # Petr√≥leo WTI
  - "GC=F"        # Ouro
  - "BDRY"        # Baltic Dry Index
  - "NG=F"        # G√°s Natural (Proxy de Fertilizante Nitrogenado)
  - "NTR"         # Nutrien Ltd (A√ß√µes de Fertilizantes)
  - "MOS"         # Mosaic Co (Proxy de Fosfatos)
  - "HE=F"        # Lean Hogs (Carne Su√≠na - Proxy Demanda China)
  - "ZM=F"        # Soybean Meal (Farelo)
  - "ZL=F"        # Soybean Oil (√ìleo)

ticker_map:
  "ZS=F": "Soja_Chicago"
  "ZC=F": "Milho_Chicago"
  "ZW=F": "Trigo_Chicago"
  "KE=F": "Trigo_Kansas"
  "USDBRL=X": "Dolar_BRL"
  "CL=F": "Petroleo_WTI"
  "GC=F": "Ouro_Risco"
  "BDRY": "Frete_Maritimo"
  "NG=F": "Gas_Natural"
  "NTR": "Acao_Nutrien"
  "MOS": "Acao_Mosaic"
  "HE=F": "Carne_Suina"
  "ZM=F": "Farelo_soja"
  "ZL=F": "Oleo_soja"

# --- 2. Par√¢metros de Risco ---
risk_thresholds:
  rsi_overbought: 70
  rsi_oversold: 30
  diesel_alert: 25
  basis_vol_multiplier: 2.0 
  logistics_peak_multiplier: 1.5 
  fertilizer_spread_alert: 20 

# --- 3. Pontos de Monitoramento Estrat√©gico ---
locations:
  # --- BRASIL CENTRO-OESTE ---
  - name: "Sorriso_MT"
    state_code: "MT"
    dist_to_port: 1500  # Dist√¢ncia cr√≠tica para regra do Diesel
    group: "BR"
    cluster: "BR_CENTER_WEST"
    lat: -12.54
    lon: -55.72
  - name: "Rio_Verde_GO"
    state_code: "GO"
    dist_to_port: 900
    group: "BR"
    cluster: "BR_CENTER_WEST"
    lat: -17.79
    lon: -50.92

  # --- BRASIL SUL (LOG√çSTICA) ---
  - name: "Cascavel_PR"
    state_code: "PR"
    dist_to_port: 500
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -24.95
    lon: -53.45
  - name: "Paranagua"
    state_code: "PR"
    dist_to_port: 0
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -25.50
    lon: -48.50
  - name: "Porto_Santos"
    state_code: "SP"
    dist_to_port: 0
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -23.96
    lon: -46.33

  # --- ARCO NORTE ---
  - name: "Itaqui_MA"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -2.57
    lon: -44.36
  - name: "Itacoatiara_AM"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -3.14
    lon: -58.44
  - name: "Santarem_PA"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -2.44
    lon: -54.70

  # --- USA CORN BELT ---
  - name: "Des_Moines_IA"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 41.60
    lon: -93.60
  - name: "Decatur_IL"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 39.84
    lon: -88.95
  - name: "Mississippi_River_St_Louis"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 38.62
    lon: -90.19

  # --- GLOBAL CHOKEPOINTS ---
  - name: "Panama_Canal"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 9.08
    lon: -79.68
  - name: "Suez_Canal"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 30.58
    lon: 32.27
  - name: "China_Dalian"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 38.91
    lon: 121.60

# --- 4. Janelas Operacionais ---
windows:
  morning: [6, 9]
  market: [11, 14]
  logistics: [15, 17]
  closing: [18, 20]

# Mapeamento de Fontes de Dados (Data Routing)
market_sources:
  default: "yahoo"
  overrides:
    "USDBRL=X": "brapi"
    "RAIL3": "brapi"
    "STBP3": "brapi"
    "PETR4": "brapi"

# Mapeamento de Tickers (Sistema Interno -> API Externa)
ticker_translation:
  brapi:
    "USDBRL=X": "USD-BRL" # Brapi usa tra√ßo
    "RAIL3": "RAIL3"
  yahoo:
    "ZS=F": "ZS=F"


============================================================
ARQUIVO: sanitize.py
LOCAL: sanitize.py
============================================================

import os
import re

# --- CONFIGURA√á√ÉO ---
ROOT_DIR = "."  # Diret√≥rio atual
DRY_RUN = False  # Se True, apenas mostra o que faria. Se False, aplica as mudan√ßas.

# Extens√µes para verificar
EXTENSIONS = {'.py', '.yaml', '.yml', '.md', '.txt'}

# Pastas para ignorar
IGNORE_DIRS = {'.git', '.venv', 'venv', '__pycache__', '.idea', '.vscode'}

# 1. Substitui√ß√µes Diretas (Case Insensitive)
# Formato: "Termo Antigo": "Termo Novo"
REPLACEMENTS = {
    r"\bMVP\b": "Projeto",
    r"\bHigh Ticket\b": "Enterprise Grade",
    r"\bSaaS\b": "System",
    r"\bVenda\b": "An√°lise",
    r"\bAssinatura\b": "Configura√ß√£o",
    r"\bPlano Beta\b": "Modo de Teste",
    r"\bComprar\b": "Acessar",
    r"Agro Risk Engine - Data Pipeline": "Agro Risk Engine - Data Pipeline",
}

# 2. Termos Sens√≠veis (Apenas avisa para voc√™ checar manualmente)
WARNING_TERMS = ["Cliente", "Pagamento", "Pre√ßo", "Lucro"]

def sanitize_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception:
        return # Pula arquivos bin√°rios ou com encoding estranho

    original_content = content
    modified = False
    
    # Aplica substitui√ß√µes
    for pattern, replacement in REPLACEMENTS.items():
        # Regex com ignore case
        new_content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
        if new_content != content:
            print(f"   [MUDAN√áA] '{pattern}' -> '{replacement}'")
            content = new_content
            modified = True

    # Remove Linhas de TODO com vi√©s comercial
    lines = content.split('\n')
    new_lines = []
    for line in lines:
        if "# TODO" in line or "# FIXME" in line:
            if any(x in line.lower() for x in ["vender", "pagamento", "Configura√ß√£o", "pre√ßo"]):
                print(f"   [REMOVIDO TODO] {line.strip()}")
                modified = True
                continue # Pula essa linha (deleta)
        new_lines.append(line)
    
    content = '\n'.join(new_lines)

    # Checa termos sens√≠veis (Cliente)
    for term in WARNING_TERMS:
        if term in content:
            print(f"   ‚ö†Ô∏è  [ATEN√á√ÉO] Termo '{term}' encontrado. Verifique o contexto manualmente.")

    # Salva se houve mudan√ßa e n√£o √© simula√ß√£o
    if modified:
        if not DRY_RUN:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"‚úÖ Arquivo atualizado: {filepath}")
        else:
            print(f"üîç [SIMULA√á√ÉO] Arquivo teria sido atualizado: {filepath}")

def main():
    print(f"üõ°Ô∏è  INICIANDO SANITIZA√á√ÉO DO PROJETO (Modo: {'SIMULA√á√ÉO' if DRY_RUN else 'GRAVA√á√ÉO'})")
    print("="*60)

    for root, dirs, files in os.walk(ROOT_DIR):
        # Filtra pastas ignoradas
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]

        for file in files:
            if any(file.endswith(ext) for ext in EXTENSIONS):
                # Pula o pr√≥prio script
                if file == "sanitize_project.py": continue
                
                filepath = os.path.join(root, file)
                print(f"\nVerificando: {filepath}...")
                sanitize_file(filepath)

    print("\n" + "="*60)
    if DRY_RUN:
        print("FIM DA SIMULA√á√ÉO. Para aplicar, mude DRY_RUN = False no script.")
    else:
        print("PROCESSO CONCLU√çDO. Seu c√≥digo est√° limpo.")

if __name__ == "__main__":
    main()


============================================================
ARQUIVO: requirements.txt
LOCAL: requirements.txt
============================================================

pandas
numpy
yfinance
requests
httpx
pydantic
fastapi
uvicorn
supabase
pyyaml
python-dotenv
feedparser
scipy
openpyxl


============================================================
ARQUIVO: README.md
LOCAL: README.md
============================================================

# Agro Risk Engine (Core) üöú

**Pipeline de Engenharia de Dados de alta performance para Modelagem de Risco de Cr√©dito Agr√≠cola.**

> **Nota:** Este reposit√≥rio √© uma extra√ß√£o refatorada do *core backend* de um Projeto comercial anterior ("AgroIntel"). O objetivo deste c√≥digo √© demonstrar pr√°ticas avan√ßadas de Engenharia de Dados (ETL Ass√≠ncrono, OOP, Design Patterns) e L√≥gica Quantitativa, isolando a intelig√™ncia do sistema das camadas de frontend legadas.

## üéØ Vis√£o Geral do Projeto

Este sistema √© um **Motor de Modelagem de Risco de Cr√©dito (Credit Risk Engine)** projetado para calcular a Probabilidade de Default (PD) em opera√ß√µes de financiamento agr√≠cola.

Diferente dos modelos tradicionais que olham apenas para o hist√≥rico financeiro est√°tico (Serasa/Bureaus), este motor incorpora fatores "invis√≠veis" e em tempo real ‚Äî como anomalias clim√°ticas, infla√ß√£o log√≠stica (Risco de Base) e correla√ß√µes macroecon√¥micas ‚Äî para gerar um score de risco din√¢mico.

**O Problema Resolvido:**
Modelos de cr√©dito tradicionais falham em detectar quando um produtor √© tecnicamente solvente, mas est√° operacionalmente quebrado devido a fatores externos. Este sistema correlaciona **Dados de Mercado Global (CBOT)**, **Custos Log√≠sticos Locais** e **Previs√µes Clim√°ticas** para prever crises de liquidez e inadimpl√™ncia antes que elas ocorram.

## üèó Arquitetura & Tech Stack

O projeto segue uma arquitetura **Modular Monolith**, utilizando ferramentas modernas para garantir reprodutibilidade, tipagem forte e performance.

*   **Linguagem:** Python 3.10+
*   **Gerenciamento de Ambiente:** [Nix](https://nixos.org/) (via `devenv`) para ambientes herm√©ticos e reprodut√≠veis.
*   **Gerenciamento de Pacotes:** [uv](https://github.com/astral-sh/uv) (escrito em Rust) para resolu√ß√£o de depend√™ncias ultra-r√°pida.
*   **Ingest√£o (ETL):** Coleta ass√≠ncrona com `httpx` e `asyncio` (L√≥gica de Retry e Backoff Exponencial).
*   **Processamento:** `Pandas` e `NumPy` para c√°lculos vetorizados de indicadores financeiros.
*   **Persist√™ncia:** PostgreSQL (via Supabase).

## üß† O "C√©rebro" do Sistema (Deep Dive no Motor)

O `core/engine.py` n√£o √© apenas um agregador de dados. Ele implementa uma **M√°quina de Estados de Risco** baseada em 4 camadas de intelig√™ncia quantitativa:

### 1. Modelagem de "Washout" (Default Estrat√©gico)
O sistema calcula a probabilidade de quebra de contrato (Washout) cruzando incentivos econ√¥micos com realidade clim√°tica.
*   **L√≥gica:** Se o pre√ßo da commodity sobe >10% (incentivo ao *default*) E a produtividade cai (falta de produto), o score de risco dispara, ignorando o hist√≥rico de cr√©dito do cliente.
*   *C√≥digo:* `core/indicators/fundamental.py` -> `calculate_washout_probability`

### 2. Proxy de Demanda Chinesa (Correla√ß√£o Cruzada)
Em vez de esperar relat√≥rios oficiais, o motor monitora o *spread* entre os futuros de **Su√≠nos (Lean Hogs)** e **Farelo de Soja**.
*   **L√≥gica:** Margem de esmagamento negativa na China antecipa queda na demanda de exporta√ß√£o brasileira semanas antes do mercado f√≠sico.
*   *C√≥digo:* `core/indicators/fundamental.py` -> `calculate_china_demand`

### 3. Sensibilidade Fenol√≥gica (Sazonalidade Din√¢mica)
O risco n√£o √© linear. O sistema aplica pesos vari√°veis dependendo do est√°gio da safra (Plantio vs. Colheita) e da regi√£o.
*   **L√≥gica:** Uma seca em Janeiro no Mato Grosso (fase cr√≠tica de enchimento de gr√£o) tem peso 5x maior no score do que uma seca em Abril.
*   *C√≥digo:* `core/seasonality.py`

### 4. Indicadores Financeiros Avan√ßados
*   **Crush Margin:** Calcula a viabilidade industrial (√ìleo + Farelo - Gr√£o) para prever demanda interna.
*   **Efeito Tesoura (Terms of Trade):** Monitora a rela√ß√£o de troca entre Receita (Soja) e Custo (Insumos/Petr√≥leo) para detectar insolv√™ncia operacional.
*   **Cisne Negro (Macro):** Detecta anomalias geopol√≠ticas via diverg√™ncia de correla√ß√£o entre Ouro e Petr√≥leo.

## üß© M√≥dulos Auxiliares

*   **Intelig√™ncia Clim√°tica (`core/climate_risk.py`):** Utiliza **Sem√°foros (Semaphores)** para controlar a concorr√™ncia das requisi√ß√µes e respeitar os *rate limits* das APIs de meteorologia.
*   **Estrat√©gias Regionais (`core/strategies/`):** Implementa√ß√£o do padr√£o *Strategy* para aplicar regras espec√≠ficas por cluster geogr√°fico (ex: MT vs PR).

## üöÄ Como Rodar o Projeto

Este projeto utiliza **uv** para gerenciamento de depend√™ncias e execu√ß√£o, garantindo velocidade e consist√™ncia.

### Pr√©-requisitos
*   [uv](https://docs.astral.sh/uv/) instalado.
*   Conta no Supabase (ou um banco PostgreSQL local).

### Instala√ß√£o e Execu√ß√£o

1. **Clone o reposit√≥rio:**
   ```bash
   git clone https://github.com/SEU_USUARIO/agro-risk-engine.git
   cd agro-risk-engine
   ```

2. **Configure as vari√°veis de ambiente:**
   Renomeie o arquivo `.env.example` para `.env` e insira suas credenciais:
   ```env
   SUPABASE_URL=sua_url_do_supabase
   SUPABASE_KEY=sua_chave_anon_ou_service
   ```

3. **Instale as depend√™ncias (via uv):**
   O `uv` cria e gerencia o ambiente virtual automaticamente.
   ```bash
   uv pip install -r requirements.txt
   ```

4. **Popule o Banco de Dados (Seed):**
   Gera dados fict√≠cios de contratos para teste de estresse e valida√ß√£o do motor.
   ```bash
   uv run python -m scripts.seed_portfolio
   ```

5. **Execute o Pipeline:**
   ```bash
   uv run python main.py --mode watch
   ```

### Op√ß√£o via Nix (Devenv)
Se voc√™ utiliza Nix, o ambiente √© configurado automaticamente com todas as depend√™ncias de sistema:
```bash
devenv shell
uv run python main.py --mode watch
```

## üìä Destaques de Engenharia

*   **Modern Tooling:** Ado√ß√£o de `uv` e `Nix` demonstra foco em *Developer Experience* (DX) e ambientes de produ√ß√£o robustos, eliminando o cl√°ssico "funciona na minha m√°quina".
*   **Defensive Coding:** O sistema possui mecanismos de *fallback* (dados sint√©ticos) e *retry* exponencial para lidar com falhas de APIs externas sem derrubar o pipeline.
*   **Audit Trail:** Cada dado ingerido √© marcado com a fonte de origem e timestamp, garantindo rastreabilidade total para auditoria de risco.

---
*Desenvolvido por Raphael Soares - Data Engineer*
```


============================================================
ARQUIVO: project_structure.txt
LOCAL: project_structure.txt
============================================================

MAPA DE ESTRUTURA E VARI√ÅVEIS DO PROJETO
Use este guia para entender a arquitetura sem ler todo o c√≥digo.

==================================================
ARQUIVO: sanitize.py
==================================================  [CONSTANTE] ROOT_DIR
  [CONSTANTE] DRY_RUN
  [CONSTANTE] EXTENSIONS
  [CONSTANTE] IGNORE_DIRS
  [CONSTANTE] REPLACEMENTS
  [CONSTANTE] WARNING_TERMS
  [FUN√á√ÉO] sanitize_file(args: filepath)
  [FUN√á√ÉO] main(args: )

==================================================
ARQUIVO: api.py
==================================================
  [CLASSE] SimulationRequest (Herda de: BaseModel)

==================================================
ARQUIVO: main.py
==================================================  [FUN√á√ÉO] main(args: )

==================================================
ARQUIVO: flow/destination.py
==================================================  [FUN√á√ÉO] get_data(args: )

==================================================
ARQUIVO: flow/export.py
==================================================  [FUN√á√ÉO] get_port_season_context(args: month)
  [FUN√á√ÉO] get_data(args: )

==================================================
ARQUIVO: flow/__init__.py
==================================================
  [ARQUIVO VAZIO]
==================================================
ARQUIVO: flow/origination.py
==================================================  [FUN√á√ÉO] get_season_context(args: month)
  [FUN√á√ÉO] get_data(args: )

==================================================
ARQUIVO: core/scout.py
==================================================
  [CLASSE] NewsScout (Herda de: )
      [M√âTODO] __init__(args: use_service_role)
      -> Atributo: self.db
      -> Atributo: self.hf_token
      -> Atributo: self.hf_api_url
      -> Atributo: self.feeds
      [M√âTODO] _generate_id(args: link)
      [M√âTODO] get_alerts(args: filter_sent)

==================================================
ARQUIVO: core/db.py
==================================================  [CONSTANTE] _CLIENT_CACHE

  [CLASSE] DatabaseManager (Herda de: )
      [M√âTODO] __init__(args: use_service_role)
      -> Atributo: self.tz
      -> Atributo: self.client
      [M√âTODO] _get_connection(args: use_service_role)
      [M√âTODO] get_active_subscribers(args: )
      [M√âTODO] should_send_email(args: subscriber_id, region, risk_data)
      [M√âTODO] log_email_sent(args: subscriber_id, region, risk_hash)
      [M√âTODO] save_risk_history(args: records)
      [M√âTODO] save_market_metrics(args: metrics)
      [M√âTODO] get_already_sent_news_ids(args: )
      [M√âTODO] mark_news_as_sent(args: news_ids)

==================================================
ARQUIVO: core/context.py
==================================================
  [CLASSE] RiskContext (Herda de: )
      [M√âTODO] __init__(args: )
      -> Atributo: self.cluster_scores
      -> Atributo: self.cluster_details
      -> Atributo: self.critical_clusters
      -> Atributo: self.pillar_sums
      -> Atributo: self.processed_count
      -> Atributo: self.avg_global_score
      -> Atributo: self.max_washout_risk
      -> Atributo: self.logistics_benchmark
      -> Atributo: self.china_metrics
      -> Atributo: self.total_exposure_brl
      -> Atributo: self.weighted_pd_sum
      -> Atributo: self.exposure_at_critical_risk
      -> Atributo: self.contract_count
      [M√âTODO] update_metrics(args: loc_name, raw_scores, metrics, climate_context)
      -> Atributo: self.max_washout_risk
      -> Atributo: self.logistics_benchmark
      [M√âTODO] register_score(args: cluster_name, loc_name, raw_scores, current_month)
      [M√âTODO] analyze_systemic_risk(args: )
      -> Atributo: self.avg_global_score
      [M√âTODO] update_portfolio_metrics(args: pd_score, loan_amount, collateral_status)
      [M√âTODO] get_portfolio_summary(args: )

==================================================
ARQUIVO: core/advisor.py
==================================================
  [CLASSE] RiskAdvisor (Herda de: )
      [M√âTODO] generate_credit_narrative(args: pd_score, metrics)

==================================================
ARQUIVO: core/market_data.py
==================================================
  [CLASSE] MarketDataError (Herda de: Exception)

  [CLASSE] MarketLoader (Herda de: )
      [M√âTODO] get_market_data(args: tickers, period)

==================================================
ARQUIVO: core/seasonality.py
==================================================
  [CLASSE] SeasonalityManager (Herda de: )
      [M√âTODO] __init__(args: )
      -> Atributo: self.state_weights
      -> Atributo: self.default_weight
      [M√âTODO] get_state_weight(args: month, state_code)
      [M√âTODO] get_weight(args: month, region_group)

  [CLASSE] RiskAnalyzer (Herda de: )
      [M√âTODO] __init__(args: raw_scores)
      -> Atributo: self.scores
      -> Atributo: self.manager
      [M√âTODO] calculate_weighted_risk(args: target_month)

==================================================
ARQUIVO: core/data_loader.py
==================================================  [FUN√á√ÉO] fetch_market_data(args: tickers, rename_map, lookback_days, timezone)

==================================================
ARQUIVO: core/validation_engine.py
==================================================
  [CLASSE] IBGEValidationEngine (Herda de: )
      [M√âTODO] __init__(args: db_manager)
      -> Atributo: self.db
      [M√âTODO] run_accuracy_test(args: simulation_tag)
      [M√âTODO] _calculate_metrics(args: df)
      [M√âTODO] _get_sim_id(args: tag)

==================================================
ARQUIVO: core/logger.py
==================================================
  [CLASSE] JsonFormatter (Herda de: )
      [M√âTODO] format(args: record)
  [FUN√á√ÉO] get_logger(args: name)

==================================================
ARQUIVO: core/engine.py
==================================================
  [CLASSE] RiskEngine (Herda de: )
      [M√âTODO] __init__(args: )
      -> Atributo: self.seasonality
      [M√âTODO] _sanitize_metrics(args: data)
      [M√âTODO] _is_data_stale(args: df)
      [M√âTODO] _calculate_calibrated_market_score(args: soy_series, usd_series)
      [M√âTODO] calculate_full_analysis(args: df_market, loc_name, df_climate, month)

  [CLASSE] OpportunityEngine (Herda de: )
      [M√âTODO] analyze_profit_windows(args: df_market, fai_status)
  [FUN√á√ÉO] calculate_market_score(args: ticker, df_market)
  [FUN√á√ÉO] _calculate_crush_margin(args: df_market)
  [FUN√á√ÉO] calculate_pd_metrics(args: df_market, loc_name, df_climate, contract_data, month)
  [FUN√á√ÉO] _calculate_dynamic_lgd(args: exposure, collateral_value)
  [FUN√á√ÉO] _calculate_ltv_exposure(args: contract, climate_score, current_price_brl, month)
  [FUN√á√ÉO] _get_empty_analysis(args: )

==================================================
ARQUIVO: core/pipeline.py
==================================================
  [CLASSE] RiskPipeline (Herda de: )
      [M√âTODO] __init__(args: mode)
      -> Atributo: self.mode
      -> Atributo: self.config
      -> Atributo: self.db
      -> Atributo: self.engine
      -> Atributo: self.climate_intel
      -> Atributo: self.scout
      -> Atributo: self.advisor
      -> Atributo: self.context
      -> Atributo: self.persister
      -> Atributo: self.br_tz
      -> Atributo: self.now_br
      -> Atributo: self.macro_corr
      -> Atributo: self.ticker_map
      [M√âTODO] run(args: )
      -> Atributo: self.contracts
      -> Atributo: self.macro_corr
      [M√âTODO] _calculate_macro_correlation(args: )
      [M√âTODO] _calculate_backtest_benchmark(args: ticker)
      [M√âTODO] _process_contracts(args: )
      [M√âTODO] _load_data(args: )
      -> Atributo: self.df_market
      -> Atributo: self.df_climate
      [M√âTODO] _extract_climate_context(args: loc_name)

==================================================
ARQUIVO: core/env.py
==================================================  [FUN√á√ÉO] load_config(args: )

  [CLASSE] EmailEnv (Herda de: )
      [M√âTODO] __init__(args: )
      -> Atributo: self.resend_key
      -> Atributo: self.sender
      -> Atributo: self.password
  [FUN√á√ÉO] load_email_env(args: )

==================================================
ARQUIVO: core/factory.py
==================================================
  [CLASSE] RegionalEngineFactory (Herda de: )
      [M√âTODO] get_strategy(args: loc_data)

  [CLASSE] RiskPipeline (Herda de: )
      [M√âTODO] __init__(args: mode, run_shadow_mode)
      -> Atributo: self.mode
      -> Atributo: self.run_shadow_mode
      -> Atributo: self.legacy_engine
      -> Atributo: self.db
      [M√âTODO] _process_regions(args: )
      [M√âTODO] _save_shadow_log(args: loc_name, legacy, current)

==================================================
ARQUIVO: core/climate_risk.py
==================================================
  [CLASSE] ClimateIntelligence (Herda de: )
      [M√âTODO] __init__(args: )
      -> Atributo: self.base_url
      -> Atributo: self.weatherapi_key
      -> Atributo: self.regions
      [M√âTODO] _get_synthetic_fallback(args: region, month)
      [M√âTODO] _is_off_season(args: region_type, hemisphere, month)
      [M√âTODO] analyze_risk(args: weather_data, region_type, hemisphere, current_month)
      [M√âTODO] run_full_scan(args: locations)

==================================================
ARQUIVO: core/brapi_client.py
==================================================
  [CLASSE] BrapiClient (Herda de: )
  [CONSTANTE] BASE_URL
      [M√âTODO] __init__(args: )
      -> Atributo: self.token
      -> Atributo: self.session
      [M√âTODO] get_historical_data(args: ticker, range_str, interval)

==================================================
ARQUIVO: core/backtest_engine.py
==================================================
  [CLASSE] InstitutionalBacktestEngine (Herda de: )
      [M√âTODO] __init__(args: risk_engine, db_manager)
      -> Atributo: self.engine
      -> Atributo: self.db
      -> Atributo: self.climate_intel
      -> Atributo: self.advisor
      [M√âTODO] run_walk_forward(args: simulation_name, start_date, end_date, contracts)
      [M√âTODO] _build_climate_snapshot(args: contracts, climate_map, current_date)
      [M√âTODO] _calculate_final_metrics_sql(args: sim_id)
      [M√âTODO] _load_historical_market(args: start, end)
      [M√âTODO] _load_historical_climate_map(args: contracts, start, end)
      [M√âTODO] _bulk_save(args: data)

==================================================
ARQUIVO: core/ibge_client.py
==================================================
  [CLASSE] IBGEClient (Herda de: )
  [CONSTANTE] BASE_URL
  [CONSTANTE] FALLBACK_DATA

==================================================
ARQUIVO: core/historical_climate_loader.py
==================================================
  [CLASSE] HistoricalClimateLoader (Herda de: )
  [CONSTANTE] ARCHIVE_URL
      [M√âTODO] __init__(args: db_manager)
      -> Atributo: self.db
      -> Atributo: self.semaphore
      [M√âTODO] _generate_hash(args: lat, lon, start, end)

==================================================
ARQUIVO: core/persister.py
==================================================
  [CLASSE] RiskPersister (Herda de: )
      [M√âTODO] __init__(args: db_manager)
      -> Atributo: self.db
      -> Atributo: self.now_iso
      [M√âTODO] save_region_risk(args: loc, final_score, raw_scores, metrics)
      [M√âTODO] save_market_metrics(args: df_market, context)
      [M√âTODO] save_global_state(args: context, macro_corr)
      [M√âTODO] save_contract_risk(args: contract, pd_score, metrics)
      [M√âTODO] _get_risk_level(args: score)

==================================================
ARQUIVO: core/market_router.py
==================================================
  [CLASSE] MarketRouter (Herda de: )
      [M√âTODO] __init__(args: config)
      -> Atributo: self.config
      -> Atributo: self.brapi
      -> Atributo: self.sources_map
      -> Atributo: self.translations
      [M√âTODO] fetch_batch(args: tickers)

==================================================
ARQUIVO: core/strategies/mt_strategy.py
==================================================
  [CLASSE] MatoGrossoStrategy (Herda de: BaseRiskStrategy)
      [M√âTODO] __init__(args: )
      -> Atributo: self.region_name
      -> Atributo: self.state_code
      [M√âTODO] calculate_logistics_risk(args: df_market, loc_data)
      [M√âTODO] calculate_climate_risk(args: df_climate, contract_data, month)
      [M√âTODO] calculate_market_risk(args: df_market)

==================================================
ARQUIVO: core/strategies/pr_strategy.py
==================================================
  [CLASSE] ParanaStrategy (Herda de: BaseRiskStrategy)
      [M√âTODO] __init__(args: )
      -> Atributo: self.region_name
      -> Atributo: self.state_code
      [M√âTODO] calculate_logistics_risk(args: df_market, loc_data)
      [M√âTODO] calculate_climate_risk(args: df_climate, contract_data, month)
      [M√âTODO] calculate_market_risk(args: df_market)

==================================================
ARQUIVO: core/strategies/base.py
==================================================
  [CLASSE] BaseRiskStrategy (Herda de: ABC)
      [M√âTODO] __init__(args: )
      -> Atributo: self.region_name
      -> Atributo: self.settings
      [M√âTODO] _load_settings(args: )
      [M√âTODO] get_data_source(args: ticker)
      [M√âTODO] translate_ticker(args: ticker, source)
      [M√âTODO] calculate_logistics_risk(args: df_market, contract_data)
      [M√âTODO] calculate_climate_risk(args: df_climate, contract_data, month)
      [M√âTODO] calculate_market_risk(args: df_market)
      [M√âTODO] get_soy_brl_price(args: df_market)
      [M√âTODO] sanitize_score(args: score)

==================================================
ARQUIVO: core/reporting/presenter.py
==================================================
  [CLASSE] HTMLPresenter (Herda de: )
      [M√âTODO] build_narrative_html(args: df_climate)

==================================================
ARQUIVO: core/indicators/macro.py
==================================================
  [CLASSE] MacroIndicators (Herda de: )
      [M√âTODO] calculate_currency_stress(args: usd_series)
      [M√âTODO] calculate_geopolitical_risk(args: gold_series, oil_series)

==================================================
ARQUIVO: core/indicators/__init__.py
==================================================
  [CLASSE] AgroIndicators (Herda de: TechnicalIndicators, FinancialIndicators, MacroIndicators)

==================================================
ARQUIVO: core/indicators/financial.py
==================================================
  [CLASSE] FinancialIndicators (Herda de: )
      [M√âTODO] theoretical_parity(args: cbot_usd_bu, usd_brl)
      [M√âTODO] calculate_soy_crush_margin(args: soy_price, meal_price, oil_price)
      [M√âTODO] calculate_market_structure(args: current_contract_price, future_contract_price)
      [M√âTODO] calculate_terms_of_trade(args: revenue_series, cost_series)
  [FUN√á√ÉO] calculate_fertilizer_affordability(args: soy_series, gas_series, stock_series)

==================================================
ARQUIVO: core/indicators/fundamental.py
==================================================
  [CLASSE] FundamentalIndicators (Herda de: )
      [M√âTODO] calculate_washout_probability(args: climate_risk_level, price_trend, price_change_30d)
      [M√âTODO] calculate_china_demand(args: soy_series, hog_series)
      [M√âTODO] calculate_basis_proxy(args: volatility, usd_ret, china_demand, is_stale, price_trend)

==================================================
ARQUIVO: core/indicators/technical.py
==================================================
  [CLASSE] TechnicalIndicators (Herda de: )
      [M√âTODO] calculate_rsi(args: series, window)
      [M√âTODO] calculate_volatility(args: series, window)
      [M√âTODO] analyze_trend(args: series, short_window, long_window)

==================================================
ARQUIVO: scripts/run_backtest.py
==================================================  [FUN√á√ÉO] _print_institutional_report(args: db, tag, real_exposure)

==================================================
ARQUIVO: scripts/ingest_historical_market.py
==================================================  [FUN√á√ÉO] ingest_historical_prices(args: )

==================================================
ARQUIVO: scripts/ingest_conab.py
==================================================
  [CLASSE] ConabExactIngestor (Herda de: )
  [CONSTANTE] TARGET_STATES
  [CONSTANTE] TARGET_COL_NAME
      [M√âTODO] __init__(args: file_path)
      -> Atributo: self.db
      -> Atributo: self.file_path
      [M√âTODO] _clean_number(args: value)
      [M√âTODO] process_file(args: )
  [CONSTANTE] FILE_NAME

==================================================
ARQUIVO: scripts/seed_portfolio.py
==================================================  [FUN√á√ÉO] seed_database(args: )

==================================================
ARQUIVO: project_mapper/project_txt.py
==================================================  [FUN√á√ÉO] transformar_projeto_definitivo(args: diretorio_raiz, arquivo_saida)

==================================================
ARQUIVO: data_ingestion/worker_market.py
==================================================  [FUN√á√ÉO] fetch_and_save(args: )



============================================================
ARQUIVO: api.py
LOCAL: api.py
============================================================

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from core.pipeline import RiskPipeline
from core.context import RiskContext
# Importe seus modelos de dados aqui

app = FastAPI(title="Agro Risk Engine DaaS", version="2.6.0-Projeto")

class SimulationRequest(BaseModel):
    portfolio_id: str
    simulation_tag: str

@app.post("/v1/risk/execute-batch")
async def run_risk_analysis(req: SimulationRequest):
    """
    Endpoint DaaS: O cliente dispara o c√°lculo e recebe o ID da execu√ß√£o.
    """
    try:
        # Reutiliza sua l√≥gica existente!
        pipeline = RiskPipeline(mode="watch")
        
        # Aqui voc√™ adaptaria o pipeline para rodar apenas para o portfolio_id espec√≠fico
        # Para o PoC, rodar o pipeline padr√£o e retornar sucesso j√° valida a integra√ß√£o.
        pipeline.run() 
        
        return {
            "status": "success",
            "message": "Risk Engine executed successfully",
            "data_source": "SANDBOX_YAHOO (Non-Commercial)", # Transpar√™ncia √© chave
            "results_url": f"/v1/results/{req.simulation_tag}"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Para rodar: uv run uvicorn api:app --reload


============================================================
ARQUIVO: main.py
LOCAL: main.py
============================================================

import argparse
import sys
from core.pipeline import RiskPipeline
from core.logger import get_logger

# Configura√ß√£o de Log
logger = get_logger("MainEntry")

def main():
    """
    Ponto de entrada da aplica√ß√£o.
    Responsabilidade: Parsear argumentos e iniciar o Pipeline.
    """
    parser = argparse.ArgumentParser(description="Agro Risk Intelligence - Execution Engine")
    parser.add_argument(
        "--mode", 
        choices=["morning", "watch"], 
        required=True, 
        help="Modo de execu√ß√£o: 'morning' (Relat√≥rio Matinal) ou 'watch' (Monitoramento Cont√≠nuo)"
    )
    args = parser.parse_args()

    try:
        # Instancia e executa o pipeline
        pipeline = RiskPipeline(mode=args.mode)
        pipeline.run()
        
    except KeyboardInterrupt:
        logger.info("üõë Execu√ß√£o interrompida pelo usu√°rio.")
        sys.exit(0)
    except Exception as e:
        logger.critical(f"‚ùå [ERRO CR√çTICO] Falha n√£o tratada no n√≠vel superior: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()


============================================================
ARQUIVO: devenv.yaml
LOCAL: devenv.yaml
============================================================

inputs:
  nixpkgs:
    url: github:cachix/devenv-nixpkgs/rolling
  nixpkgs-python:
    url: github:cachix/nixpkgs-python
    inputs:
      nixpkgs:
        follows: nixpkgs



============================================================
ARQUIVO: settings.yaml
LOCAL: configs/settings.yaml
============================================================

# ==============================================================================
# üåæ Agro Risk Engine - Data Pipeline CONFIGURATION (UNIFIED)
# ==============================================================================
project_name: "Agro Risk Engine - Data Pipeline"
version: "2.6.0"

# --- 1. Market Data (Ativos) ---
tickers:
  - "ZS=F"        # Soja Chicago (Contrato Atual/Pr√≥ximo)
  - "ZC=F"        # Milho Chicago
  - "ZW=F"        # Trigo Chicago
  - "KE=F"        # Trigo Kansas
  - "USDBRL=X"    # D√≥lar
  - "CL=F"        # Petr√≥leo WTI
  - "GC=F"        # Ouro
  - "BDRY"        # Baltic Dry Index
  - "NG=F"        # G√°s Natural (Proxy de Fertilizante Nitrogenado)
  - "NTR"         # Nutrien Ltd (A√ß√µes de Fertilizantes)
  - "MOS"         # Mosaic Co (Proxy de Fosfatos)
  - "HE=F"        # Lean Hogs (Carne Su√≠na - Proxy Demanda China)
  - "ZM=F"        # Soybean Meal (Farelo)
  - "ZL=F"        # Soybean Oil (√ìleo)

ticker_map:
  "ZS=F": "Soja_Chicago"
  "ZC=F": "Milho_Chicago"
  "ZW=F": "Trigo_Chicago"
  "KE=F": "Trigo_Kansas"
  "USDBRL=X": "Dolar_BRL"
  "CL=F": "Petroleo_WTI"
  "GC=F": "Ouro_Risco"
  "BDRY": "Frete_Maritimo"
  "NG=F": "Gas_Natural"
  "NTR": "Acao_Nutrien"
  "MOS": "Acao_Mosaic"
  "HE=F": "Carne_Suina"
  "ZM=F": "Farelo_soja"
  "ZL=F": "Oleo_soja"

# --- 2. Par√¢metros de Risco ---
risk_thresholds:
  rsi_overbought: 70
  rsi_oversold: 30
  diesel_alert: 25
  basis_vol_multiplier: 2.0 
  logistics_peak_multiplier: 1.5 
  fertilizer_spread_alert: 20 

# --- 3. Pontos de Monitoramento Estrat√©gico ---
locations:
  # --- BRASIL CENTRO-OESTE ---
  - name: "Sorriso_MT"
    state_code: "MT"
    dist_to_port: 1500  # Dist√¢ncia cr√≠tica para regra do Diesel
    group: "BR"
    cluster: "BR_CENTER_WEST"
    lat: -12.54
    lon: -55.72
  - name: "Rio_Verde_GO"
    state_code: "GO"
    dist_to_port: 900
    group: "BR"
    cluster: "BR_CENTER_WEST"
    lat: -17.79
    lon: -50.92

  # --- BRASIL SUL (LOG√çSTICA) ---
  - name: "Cascavel_PR"
    state_code: "PR"
    dist_to_port: 500
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -24.95
    lon: -53.45
  - name: "Paranagua"
    state_code: "PR"
    dist_to_port: 0
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -25.50
    lon: -48.50
  - name: "Porto_Santos"
    state_code: "SP"
    dist_to_port: 0
    group: "BR"
    cluster: "BR_SOUTH"
    lat: -23.96
    lon: -46.33

  # --- ARCO NORTE ---
  - name: "Itaqui_MA"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -2.57
    lon: -44.36
  - name: "Itacoatiara_AM"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -3.14
    lon: -58.44
  - name: "Santarem_PA"
    group: "BR"
    cluster: "BR_NORTH_ARC"
    lat: -2.44
    lon: -54.70

  # --- USA CORN BELT ---
  - name: "Des_Moines_IA"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 41.60
    lon: -93.60
  - name: "Decatur_IL"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 39.84
    lon: -88.95
  - name: "Mississippi_River_St_Louis"
    group: "US"
    cluster: "US_CORN_BELT"
    lat: 38.62
    lon: -90.19

  # --- GLOBAL CHOKEPOINTS ---
  - name: "Panama_Canal"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 9.08
    lon: -79.68
  - name: "Suez_Canal"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 30.58
    lon: 32.27
  - name: "China_Dalian"
    group: "GLOBAL"
    cluster: "GLOBAL_CHOKEPOINTS"
    lat: 38.91
    lon: 121.60

# --- 4. Janelas Operacionais ---
windows:
  morning: [6, 9]
  market: [11, 14]
  logistics: [15, 17]
  closing: [18, 20]

# Mapeamento de Fontes de Dados (Data Routing)
market_sources:
  default: "yahoo"
  overrides:
    "USDBRL=X": "brapi"
    "RAIL3": "brapi"
    "STBP3": "brapi"
    "PETR4": "brapi"

# Mapeamento de Tickers (Sistema Interno -> API Externa)
ticker_translation:
  brapi:
    "USDBRL=X": "USD-BRL" # Brapi usa tra√ßo
    "RAIL3": "RAIL3"
  yahoo:
    "ZS=F": "ZS=F"


============================================================
ARQUIVO: TECH_REFERENCE.MD
LOCAL: docs/TECH_REFERENCE.MD
============================================================

Esta √© a **Documenta√ß√£o T√©cnica de Refer√™ncia (Technical Reference Guide)** do sistema **Agro Risk Engine - Data Pipeline (v2.6.0)**. Este documento foi estruturado para servir como guia definitivo para transfer√™ncia de propriedade, auditoria e manuten√ß√£o futura.

---

# üõ∞Ô∏è Agro Risk Monitor - Technical Reference Guide

## 1. Vis√£o Geral da Arquitetura
O sistema √© uma plataforma de intelig√™ncia de risco agr√≠cola baseada em dados, projetada para monitorar vari√°veis clim√°ticas, log√≠sticas e de mercado. A arquitetura segue o padr√£o **Modular Monolith** com separa√ß√£o clara entre ingest√£o de dados, processamento de risco e interface de visualiza√ß√£o.

### Fluxo de Dados Macro:
1.  **Ingest√£o (Ingestion Layer):** Scripts de coleta (`worker_market.py`, `flow/`) buscam dados em tempo real de APIs externas (Yahoo Finance, Open-Meteo, Google News).
2.  **Persist√™ncia (Storage Layer):** Os dados brutos e processados s√£o armazenados no **Supabase** (PostgreSQL).
3.  **Processamento (Intelligence Engine):** O `RiskPipeline` orquestra a an√°lise. Ele utiliza o **Strategy Pattern** para aplicar regras de risco espec√≠ficas por regi√£o (MT, PR) e o **Facade Pattern** para indicadores t√©cnicos e financeiros.
4.  **Sa√≠da (Output Layer):**
    *   **Dashboard:** Interface Streamlit para visualiza√ß√£o em tempo real.
    *   **Relat√≥rios:** Gera√ß√£o de PDFs via `FPDF` e envio de e-mails via `MailerSend`.

---

## 2. Dicion√°rio de Scripts e M√≥dulos

### üìÇ Core (O Cora√ß√£o do Sistema)
*   **`core/pipeline.py`**: Orquestrador principal. Coordena a carga de dados, execu√ß√£o dos motores de risco e disparo de notifica√ß√µes.
*   **`core/engine.py`**: Motor de risco gen√©rico. Cont√©m a l√≥gica de sanitiza√ß√£o de dados e c√°lculos de volatilidade/tend√™ncia.
*   **`core/db.py`**: Gerenciador de conex√£o com o Supabase. Implementa cache de conex√£o para evitar exaust√£o de recursos.
*   **`core/climate_risk.py`**: Interface com APIs de clima. Possui l√≥gica de *fallback* sint√©tico caso as APIs falhem.
*   **`core/scout.py`**: M√≥dulo de OSINT (Open Source Intelligence). Usa IA (Hugging Face) para classificar not√≠cias em "Cr√≠tico", "Oportunidade" ou "Neutro".
*   **`core/report_generator.py`**: Respons√°vel pela constru√ß√£o visual do PDF, incluindo gera√ß√£o de gr√°ficos via Matplotlib.

### üìÇ Interface (Frontend)
*   **`dashboard.py`**: Ponto de entrada da interface Streamlit.
*   **`interface/auth.py`**: Gerenciamento de sess√£o e integra√ß√£o com Supabase Auth.
*   **`interface/pages.py`**: L√≥gica de renderiza√ß√£o das abas (Vis√£o Geral, Mercado, Clima).
*   **`interface/components.py`**: Componentes visuais reutiliz√°veis (Cards de KPI, Gr√°ficos Plotly).

### üìÇ Flow & Workers (Automa√ß√£o)
*   **`worker_market.py`**: Script independente para coleta de pre√ßos de commodities e c√¢mbio.
*   **`flow/origination.py` / `export.py`**: L√≥gicas espec√≠ficas para monitorar o interior (fazendas) e os portos.

---

## 3. Detalhamento de C√≥digo

### Classes Principais

| Classe | Responsabilidade | Heran√ßa/Depend√™ncia |
| :--- | :--- | :--- |
| `RiskPipeline` | Coordena o ciclo de vida da an√°lise de risco. | `DatabaseManager`, `RiskEngine` |
| `BaseRiskStrategy` | Classe abstrata que define a interface para riscos regionais. | `ABC` |
| `MatoGrossoStrategy` | Implementa regras espec√≠ficas (ex: Regra do Diesel/Dist√¢ncia). | `BaseRiskStrategy` |
| `DatabaseManager` | Singleton para opera√ß√µes de CRUD no Supabase. | `supabase-py` |
| `NewsScout` | Coleta e analisa not√≠cias via RSS e NLP. | `httpx`, `HuggingFace API` |

### Fun√ß√µes e M√©todos Cr√≠ticos

#### `RiskEngine.calculate_full_analysis`
*   **O que faz:** Realiza o cruzamento de dados de mercado, clima e log√≠stica para um local espec√≠fico.
*   **Par√¢metros:** `df_market` (DataFrame), `loc_name` (String), `df_climate` (DataFrame).
*   **Retorno:** Um dicion√°rio de scores (0-100) e um dicion√°rio de m√©tricas detalhadas.

#### `FundamentalIndicators.calculate_washout_probability`
*   **O que faz:** Calcula a probabilidade de quebra de contrato (default) baseada no estresse clim√°tico e varia√ß√£o de pre√ßo.
*   **L√≥gica:** Se o pre√ßo sobe muito (incentivo ao default) e o clima √© cr√≠tico (falta de produto), o score atinge o n√≠vel m√°ximo.

#### `ClimateIntelligence.run_full_scan`
*   **O que faz:** Varredura ass√≠ncrona de m√∫ltiplos pontos geogr√°ficos.
*   **Diferencial:** Implementa um **Sem√°foro de Concorr√™ncia** para n√£o ser bloqueado pelas APIs de clima.

---

## 4. Vari√°veis Cr√≠ticas e Configura√ß√µes

### Arquivo `configs/settings.yaml`
Este √© o c√©rebro configur√°vel do projeto.
*   **`tickers`**: Lista de ativos monitorados (ZS=F para Soja, USDBRL=X para D√≥lar).
*   **`risk_thresholds`**: Gatilhos num√©ricos (ex: `rsi_overbought: 70`).
*   **`locations`**: Lista de dicion√°rios contendo lat/lon e `dist_to_port` (usado no c√°lculo de frete).

### Vari√°veis de Ambiente (`.env`)
*   `SUPABASE_URL` / `SUPABASE_SERVICE_ROLE_KEY`: Acesso ao banco de dados.
*   `HUGGINGFACE_API_KEY`: Token para o modelo de classifica√ß√£o de not√≠cias.
*   `MAILERSEND_API_KEY`: Token para envio de e-mails transacionais.

---

## 5. Mapeamento de Rela√ß√µes
1.  **`main.py`** chama **`RiskPipeline`**.
2.  **`RiskPipeline`** consome **`MarketLoader`** para obter dados de pre√ßos.
3.  **`RiskPipeline`** instancia a estrat√©gia correta via **`RegionalEngineFactory`**.
4.  A estrat√©gia (ex: **`ParanaStrategy`**) usa **`TechnicalIndicators`** para validar a volatilidade.
5.  O resultado √© enviado para o **`RiskPersister`**, que grava no Supabase.
6.  O **`RiskNotifier`** verifica se o score global ultrapassou o limite e aciona o **`MailerSend`**.

---

## 6. Guia de Manuten√ß√£o

### Como adicionar uma nova regi√£o de monitoramento?
1.  Crie uma nova classe em `core/strategies/` herdando de `BaseRiskStrategy`.
2.  Implemente os m√©todos de c√°lculo de risco log√≠stico, clim√°tico e de mercado.
3.  Registre a nova regi√£o no `RegionalEngineFactory` dentro de `core/factory.py`.

### Como alterar a sensibilidade do risco de mercado?
*   V√° at√© `core/engine.py` no m√©todo `_calculate_calibrated_market_score`.
*   Ajuste os pesos de `vol_score` e `price_risk`.

### Como modificar o layout do e-mail?
*   Altere os templates HTML em `core/reporting_template.py`.

---

## 7. Depend√™ncias Externas e APIs
*   **Yahoo Finance (yfinance):** Dados hist√≥ricos e tempo real de commodities.
*   **Open-Meteo API:** Previs√£o de precipita√ß√£o e temperatura (Gratuito/Open Source).
*   **WeatherAPI:** Fallback secund√°rio para dados clim√°ticos.
*   **Supabase:** Backend-as-a-Service (PostgreSQL, Auth, Realtime).
*   **Hugging Face (BART Model):** Classifica√ß√£o de texto (Zero-shot classification) para not√≠cias.
*   **MailerSend:** Infraestrutura de envio de e-mails.

---

**Nota de Seguran√ßa:** Este sistema utiliza chaves de API sens√≠veis. Certifique-se de que o arquivo `.env` nunca seja versionado em reposit√≥rios p√∫blicos. A integridade dos dados depende da execu√ß√£o regular do `worker_market.py`.


============================================================
ARQUIVO: destination.py
LOCAL: flow/destination.py
============================================================

import yfinance as yf
import pandas as pd
import requests
from datetime import datetime
from core.logger import get_logger

logger = get_logger(__name__)

def get_data():
    """
    VIS√ÉO ESTRAT√âGICA: Coleta de indicadores financeiros e gargalos log√≠sticos globais.
    """
    month = datetime.now().month
    logger.info("Iniciando coleta de dados globais", extra={"context": "destination"})
    
    # 1. FINANCEIRO (Moedas de Importa√ß√£o/Exporta√ß√£o)
    risk_metrics = {"eur_ret": None, "cny_ret": None}
    
    try:
        tickers = ['EURUSD=X', 'CNY=X']
        # Timeout e tratamento de dados vazios para resili√™ncia
        data = yf.download(tickers, period='5d', progress=False, timeout=10)['Close']
        
        if not data.empty and len(data) > 1:
            # pct_change(fill_method=None) evita avisos de deprecia√ß√£o do pandas
            returns = data.pct_change(fill_method=None).iloc[-1]
            risk_metrics["eur_ret"] = float(returns['EURUSD=X'])
            risk_metrics["cny_ret"] = float(returns['CNY=X'])
        else:
            logger.warning("Yahoo Finance retornou dados insuficientes para c√°lculo de retorno.")

    except Exception as e:
        logger.error(f"Falha ao obter dados financeiros: {str(e)}", exc_info=True)

    # 2. LOCAIS ESTRAT√âGICOS (Gargalos de Pre√ßo e Insumo)
    locations = {
        "China_Dalian": {"lat": 38.91, "lon": 121.60, "type": "Buyer", "desc": "China (Porto Dalian)"},
        "Panama_Canal": {"lat": 9.08, "lon": -79.68, "type": "Logistics_Global", "desc": "Canal do Panam√°"},
        "Suez_Canal": {"lat": 30.58, "lon": 32.27, "type": "Inputs", "desc": "Canal de Suez (Insumos)"}
    }
    
    climate_rows = []

    for name, coords in locations.items():
        try:
            url = f"https://api.open-meteo.com/v1/forecast?latitude={coords['lat']}&longitude={coords['lon']}&daily=precipitation_sum,wind_speed_10m_max&timezone=UTC"
            
            # Timeout √© crucial para n√£o travar a pipeline
            res = requests.get(url, timeout=5)
            res.raise_for_status()
            
            data = res.json()
            daily = data.get('daily', {})
            
            # Extra√ß√£o segura dos valores (primeiro dia da previs√£o)
            rain = daily.get('precipitation_sum', [0])[0] if daily.get('precipitation_sum') else 0
            wind = daily.get('wind_speed_10m_max', [0])[0] if daily.get('wind_speed_10m_max') else 0
            
            status = "üü¢ OPERA√á√ÉO NORMAL"
            
            # REGRAS DE RISCO NEGOCIAL
            if "China" in name and wind > 60: 
                status = "üî¥ PORTO FECHADO (VENTO)"
            elif "Panama" in name and month <= 5 and rain < 5: 
                status = "üü° N√çVEL BAIXO (FRETE CARO)"
            elif "Suez" in name and wind > 60: 
                status = "üü° TEMPESTADE AREIA (ATRASO)"
            
            climate_rows.append({
                "Location": name, 
                "Risk_Status": status, 
                "Category": "Global",
                "Description": coords['desc']
            })
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Erro de conex√£o/API Clima para {name}: {e}")
            climate_rows.append({
                "Location": name, 
                "Risk_Status": "‚ö™ DADOS INDISPON√çVEIS (API ERROR)", 
                "Category": "Global",
                "Description": coords['desc']
            })
        except Exception as e:
            logger.error(f"Erro inesperado ao processar {name}: {e}", exc_info=True)

    return {"risk_metrics": risk_metrics, "alerts": []}, pd.DataFrame(climate_rows)


============================================================
ARQUIVO: export.py
LOCAL: flow/export.py
============================================================

import yfinance as yf
import pandas as pd
import requests
import feedparser
from datetime import datetime

def get_port_season_context(month):
    """Define o n√≠vel de estresse log√≠stico nos portos BR."""
    if 2 <= month <= 6: return "ALTA TEMPORADA (SOJA) - FILA M√ÅXIMA"
    elif 7 <= month <= 9: return "ALTA TEMPORADA (MILHO) - FILA INTENSA"
    else: return "ENTRESSAFRA (MANUTEN√á√ÉO/BAIXA)"

def get_data():
    """
    FOCO: Exporta√ß√£o (Portos BR).
    Monitora: Santos (Sudeste), Paranagu√° (Sul) e Itaqui (Arco Norte).
    Sens√≠vel ao Line-up (Fila de Navios) da √©poca.
    """
    month = datetime.now().month
    season_context = get_port_season_context(month)
    print(f"üö¢ Coletando Exporta√ß√£o... [Contexto: {season_context}]")
    
    # Define se √© √©poca cr√≠tica de embarque (Line-up cheio)
    is_peak_season = 2 <= month <= 9 

# 1. FINANCEIRO (Basis/Pr√™mio & C√¢mbio)
    risk_metrics = {"diff_basis": 0.0, "usd_ret": 0.0}
    try:
        tickers = ['BRL=X', 'ZS=F']
        data = yf.download(tickers, period='5d', progress=False)['Close']
        
        if not data.empty:
            # CORRE√á√ÉO: Usamos .values[-1] aqui tamb√©m
            risk_metrics["usd_ret"] = float(data['BRL=X'].pct_change().values[-1])
            
            # Para o Basis (Volatilidade)
            soy_vol = data['ZS=F'].pct_change(fill_method=None).std()
            risk_metrics["diff_basis"] = float(soy_vol * 2.0)
    except Exception: pass

    # 2. NOT√çCIAS (Focada em Line-up e Demurrage)
    alerts = []
    try:
        # Busca por filas, multas e problemas portu√°rios
        feed = feedparser.parse("https://news.google.com/rss/search?q=porto+santos+paranagua+fila+navios+demurrage+arco+norte&hl=pt-BR&gl=BR&ceid=BR:pt-419")
        for entry in feed.entries[:3]: alerts.append(entry.title.upper())
    except Exception: pass

    # 3. CLIMA PORTU√ÅRIO
    # Monitoramos os 3 principais exaustores do Brasil
    ports = {
        "Santos_SP": {"lat": -23.96, "lon": -46.33, "type": "Main_Hub"},
        "Paranagua_PR": {"lat": -25.51, "lon": -48.50, "type": "Main_Hub"},
        "Itaqui_MA": {"lat": -2.57, "lon": -44.36, "type": "North_Arc"} # Sa√≠da do Arco Norte
    }
    
    climate_rows = []
    for name, coords in ports.items():
        try:
            url = f"https://api.open-meteo.com/v1/forecast?latitude={coords['lat']}&longitude={coords['lon']}&daily=precipitation_sum,wind_speed_10m_max&timezone=America%2FSao_Paulo"
            res = requests.get(url, timeout=5).json()
            daily = res.get('daily', {})
            rain = daily.get('precipitation_sum', [0])[0]
            wind = daily.get('wind_speed_10m_max', [0])[0]
            
            status = "üü¢ OPERA√á√ÉO NORMAL"
            
            # --- L√ìGICA DE PORTO (CHUVA = HATCH CLOSED) ---
            
            # Regra de Chuva (Paralisa embarque)
            if rain > 10.0:
                if is_peak_season:
                    # Na alta temporada, chuva gera multas (Demurrage) pesadas
                    status = "üî¥ CHUVA/LINE-UP (DEMURRAGE RISK)"
                else:
                    status = "üü° ATRASO OPERACIONAL (CHUVA)"
            
            # Regra de Vento (Gruas param)
            if wind > 45.0:
                status = "üî¥ VENDAVAL (GRUAS PARADAS)"

            climate_rows.append({"Location": name, "Risk_Status": status, "Category": "Export"})
        except Exception:
            climate_rows.append({"Location": name, "Risk_Status": "‚ö™ DADOS INDISPON√çVEIS", "Category": "Export"})

    return {"risk_metrics": risk_metrics, "alerts": alerts}, pd.DataFrame(climate_rows)


============================================================
ARQUIVO: __init__.py
LOCAL: flow/__init__.py
============================================================




============================================================
ARQUIVO: origination.py
LOCAL: flow/origination.py
============================================================

import yfinance as yf
import pandas as pd
import requests
import feedparser
from datetime import datetime

def get_season_context(month):
    """Retorna o contexto agr√≠cola do m√™s atual para logging."""
    if 1 <= month <= 3: return "COLHEITA (SOJA) & LOG√çSTICA"
    elif 4 <= month <= 5: return "DESENVOLVIMENTO (MILHO SAFRINHA)"
    elif 6 <= month <= 9: return "ESCOAMENTO & NAVEGA√á√ÉO (ARCO NORTE)"
    else: return "PLANTIO (SAFRA NOVA)"

def get_data():
    """
    FOCO: Origina√ß√£o Inteligente com Matriz Sazonal Completa.
    Monitora: Sorriso (Fazenda), Sinop (Rodovia), Santar√©m (Hidrovia).
    """
    month = datetime.now().month
    season_name = get_season_context(month)
    print(f"üöú Coletando Origina√ß√£o... [Fase: {season_name}]")
    
# 1. FINANCEIRO (Diesel & Custos)
    risk_metrics = {"diesel_proxy": 0.0}
    try:
        # Petr√≥leo Brent como proxy do Diesel
        brent = yf.download('BZ=F', period='5d', progress=False)['Close']
        if not brent.empty:
            # CORRE√á√ÉO: Usamos .values[-1] para pegar o valor num√©rico direto
            risk_metrics["diesel_proxy"] = float(brent.pct_change().values[-1])
    except Exception: pass

    # 2. NOT√çCIAS (Query adapt√°vel baseada na fase)
    alerts = []
    query_map = {
        1: "soja atraso colheita mato grosso chuva grao ardido",
        2: "soja atraso colheita br-163 atoleiro",
        3: "soja produtividade colheita final",
        9: "baixo nivel rio tapajos santarem barcaca",
        10: "atraso plantio soja falta chuva mato grosso"
    }
    # Pega a query do m√™s atual ou usa uma gen√©rica
    query = query_map.get(month, "agronegocio logistica soja milho brasil")
    
    try:
        rss_url = f"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=pt-BR&gl=BR&ceid=BR:pt-419"
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:3]: alerts.append(entry.title.upper())
    except Exception: pass

    # 3. CLIMA & L√ìGICA DE RISCO (A MATRIZ DE DECIS√ÉO)
    locations = {
        "Sorriso_MT": {"lat": -12.54, "lon": -55.72, "type": "Farm_Soy"},
        "Londrina_PR": {"lat": -23.30, "lon": -51.16, "type": "Farm_Soy"},
        "Sinop_MT_BR163": {"lat": -11.86, "lon": -55.50, "type": "Road"},
        "Santarem_PA": {"lat": -2.44, "lon": -54.70, "type": "River"}
    }
    
    climate_rows = []
    
    for name, coords in locations.items():
        try:
            # Pega Chuva (mm) e Temperatura Max (¬∞C)
            url = f"https://api.open-meteo.com/v1/forecast?latitude={coords['lat']}&longitude={coords['lon']}&daily=precipitation_sum,temperature_2m_max&timezone=America%2FSao_Paulo"
            res = requests.get(url, timeout=5).json()
            daily = res.get('daily', {})
            rain = daily.get('precipitation_sum', [0])[0]
            temp = daily.get('temperature_2m_max', [0])[0]
            
            status = "üü¢ OPERA√á√ÉO NORMAL"
            
            # --- MATRIZ DE DECIS√ÉO SAZONAL ---
            
            # [Q1] JANEIRO A MAR√áO: COLHEITA (Chuva = Ruim)
            if 1 <= month <= 3:
                if coords['type'] == "Farm_Soy":
                    if rain > 30.0: status = "üî¥ PARADA TOTAL (RISCO GR√ÉO ARDIDO)"
                    elif rain > 10.0: status = "üü° UMIDADE ALTA (COLHEITA LENTA)"
                elif coords['type'] == "Road":
                    if rain > 40.0: status = "üî¥ BLOQUEIO BR-163 (LAMA)"
                    elif rain > 20.0: status = "üü° TR√ÅFEGO LENTO"
                elif coords['type'] == "River":
                    if rain > 15.0: status = "üü° CARREGAMENTO LENTO (CHUVA)"

            # [Q2] ABRIL A MAIO: SAFRINHA MILHO (Seca = Ruim)
            elif 4 <= month <= 5:
                if coords['type'] == "Farm_Soy": # Aqui j√° √© Milho na terra
                    if rain < 2.0: status = "üî¥ ESTRESSE H√çDRICO (QUEBRA MILHO)"
                    elif rain < 10.0: status = "üü° ALERTA DE SECA"

            # [Q3] JUNHO A SETEMBRO: LOG√çSTICA FLUVIAL (Seca Rios = Ruim)
            elif 6 <= month <= 9:
                if coords['type'] == "River": # Foco total em Santar√©m
                    if rain < 5.0: status = "üî¥ RIO BAIXO (BARCA√áAS PARADAS)"
                    elif rain < 15.0: status = "üü° CALADO REDUZIDO"
                elif coords['type'] == "Road":
                    status = "üü¢ ESTRADA SECA (√ìTIMO FLUXO)"

            # [Q4] OUTUBRO A DEZEMBRO: PLANTIO (Seca = Ruim)
            else: 
                if coords['type'] == "Farm_Soy":
                    if rain < 5.0: status = "üî¥ PLANTIO PARADO (SOLO SECO)"
                    elif rain < 15.0: status = "üü° ATRASO NO PLANTIO"
                    elif rain > 50.0: status = "üü° EXCESSO DE CHUVA (LAVAGEM)" # Muita chuva lava a semente
                elif coords['type'] == "River":
                     status = "üü¢ N√çVEL EM RECUPERA√á√ÉO"

            climate_rows.append({"Location": name, "Risk_Status": status, "Category": "Origination"})
            
        except Exception:
            climate_rows.append({"Location": name, "Risk_Status": "‚ö™ DADOS INDISPON√çVEIS", "Category": "Origination"})

    return {"risk_metrics": risk_metrics, "alerts": alerts}, pd.DataFrame(climate_rows)


============================================================
ARQUIVO: scout.py
LOCAL: core/scout.py
============================================================

# ARQUIVO: core/scout.py
import feedparser
import hashlib
import logging
import os
import httpx
import traceback
from datetime import datetime, timedelta
from core.db import DatabaseManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("NewsScout")

class NewsScout:
    """
    OSINT v4.8 - Otimizado para evitar vazamento de conex√µes.
    """
    
    def __init__(self, use_service_role=False):
        self.db = DatabaseManager(use_service_role=use_service_role)
        self.hf_token = os.getenv("HUGGINGFACE_API_KEY")
        self.hf_api_url = "https://router.huggingface.co/hf-inference/models/facebook/bart-large-mnli"
        
        self.feeds = {
            'logistica_br': 'https://news.google.com/rss/search?q=greve+caminhoneiros+OR+bloqueio+br-163+OR+porto+santos+paralisacao&hl=pt-BR&gl=BR&ceid=BR:pt-419',
            'geopolitica': 'https://news.google.com/rss/search?q=suez+canal+blocked+OR+panama+canal+drought+OR+war+trade+routes&hl=en-US&gl=US&ceid=US:en',
            'mercado': 'https://news.google.com/rss/search?q=soja+recorde+safra+OR+milho+exportacao+brasil&hl=pt-BR&gl=BR&ceid=BR:pt-419'
        }

    def _generate_id(self, link):
        return hashlib.md5(link.encode('utf-8')).hexdigest()

    async def _analyze_with_ai(self, text, client: httpx.AsyncClient):
        """Analisa o texto usando um cliente HTTP j√° existente."""
        if not self.hf_token:
            return "NEUTRO", 0.0

        headers = {"Authorization": f"Bearer {self.hf_token}"}
        payload = {
            "inputs": text,
            "parameters": {"candidate_labels": ["Supply Chain Crisis", "Market Opportunity", "Irrelevant News", "Weather Disaster"]}
        }

        try:
            # REUTILIZA o cliente passado por par√¢metro
            response = await client.post(self.hf_api_url, headers=headers, json=payload, timeout=20.0)
            
            if response.status_code != 200:
                logger.warning(f"‚ö†Ô∏è HF API Error: {response.status_code}")
                return "NEUTRO", 0.0

            result = response.json()
            data = result[0] if isinstance(result, list) else result
            
            if 'labels' in data and 'scores' in data:
                return data['labels'][0], data['scores'][0]
            return "NEUTRO", 0.0

        except Exception:
            logger.error(f"‚ùå Erro na an√°lise de IA: {traceback.format_exc()}")
            return "NEUTRO", 0.0

    async def fetch_and_store(self):
        """Varredura principal com gerenciamento eficiente de conex√µes."""
        logger.info("üïµÔ∏è Scout AI: Iniciando varredura inteligente...")
        
        # O 'async with' fora do loop garante que apenas UM cliente seja usado para tudo
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as http_client:
            for category, url in self.feeds.items():
                try:
                    response = await http_client.get(url)
                    feed = feedparser.parse(response.text)
                    
                    for entry in feed.entries[:5]:
                        alert_id = self._generate_id(entry.link)
                        
                        # Passamos o http_client para a fun√ß√£o de IA
                        label, score = await self._analyze_with_ai(entry.title, http_client)
                        
                        risk_level = "NEUTRO"
                        if score > 0.5:
                            if label in ["Supply Chain Crisis", "Weather Disaster"]:
                                risk_level = "CR√çTICO"
                            elif label == "Market Opportunity":
                                risk_level = "OPORTUNIDADE"
                        
                        if risk_level != "NEUTRO":
                            # Usa o fuso hor√°rio do DatabaseManager (Bras√≠lia)
                            now = datetime.now(self.db.tz)
                            payload = {
                                "id": alert_id,
                                "title": entry.title,
                                "category": category,
                                "risk_level": risk_level,
                                "source_url": entry.link,
                                "expires_at": (now + timedelta(hours=24)).isoformat(),
                                "created_at": now.isoformat()
                            }
                            
                            try:
                                self.db.client.table('scout_cache').upsert(payload).execute()
                                logger.info(f"üß† AI ({risk_level}): {entry.title[:40]}...")
                            except Exception as e:
                                logger.error(f"Erro ao salvar no banco: {e}")

                except Exception as e:
                    logger.error(f"Erro ao processar feed {category}: {e}")
                    continue

    def get_alerts(self, filter_sent=True):
        """Recupera alertas v√°lidos do banco."""
        if not self.db.client: return []
        try:
            now = datetime.now(self.db.tz).isoformat()
            sent_ids = self.db.get_already_sent_news_ids() if filter_sent else []

            res = self.db.client.table('scout_cache')\
                .select("*")\
                .gt('expires_at', now)\
                .order('created_at', desc=True)\
                .execute()
            
            all_alerts = res.data if res.data else []

            if filter_sent and sent_ids:
                return [a for a in all_alerts if a['id'] not in sent_ids]
            
            return all_alerts

        except Exception as e:
            logger.error(f"Erro ao recuperar alertas: {e}")
            return []


============================================================
ARQUIVO: db.py
LOCAL: core/db.py
============================================================

# ARQUIVO: core/db.py
import os
import logging
import pytz
import time
from datetime import datetime  # Adicionado para suportar o novo m√©todo
from dotenv import load_dotenv
from supabase import create_client, Client

# Configura√ß√£o de Log
logger = logging.getLogger(__name__)
load_dotenv()

# --- CACHE GLOBAL DE CONEX√ïES ---
# Isso garante que o Python s√≥ abra UMA conex√£o por tipo, resolvendo o "Device busy"
_CLIENT_CACHE = {
    "service": None,
    "anon": None
}

class DatabaseManager:
    def __init__(self, use_service_role: bool = False):
        self.tz = pytz.timezone('America/Sao_Paulo')
        self.client = self._get_connection(use_service_role)

    def _get_connection(self, use_service_role: bool) -> Client:
        """
        Recupera a conex√£o do cache global ou cria uma nova.
        """
        key_type = "service" if use_service_role else "anon"
        
        # 1. Se j√° existe no cache, usa ela (Zero custo de conex√£o)
        if _CLIENT_CACHE[key_type] is not None:
            return _CLIENT_CACHE[key_type]

        # 2. Configura√ß√£o de Credenciais
        url = os.getenv("SUPABASE_URL")
        if use_service_role:
            key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
            mode_label = "ALTO PRIVIL√âGIO (Service Role)"
        else:
            key = os.getenv("SUPABASE_KEY")
            mode_label = "PRIVIL√âGIO LIMITADO (Anon)"

        if not url or not key:
            logger.critical(f"‚ùå Credenciais ausentes para: {mode_label}")
            return None

        # 3. Tentativa de Conex√£o com Retry Simples
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logger.info(f"üîå Reconectando Supabase ({mode_label})... Tentativa {attempt+1}")
                
                # Usamos o padr√£o da biblioteca, que √© mais seguro
                client = create_client(url, key)
                
                # Salva no cache global
                _CLIENT_CACHE[key_type] = client
                logger.info(f"‚úÖ Conex√£o estabelecida e cacheada: {mode_label}")
                return client

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha na conex√£o (Tentativa {attempt+1}): {e}")
                time.sleep(2) # Espera um pouco para o sistema liberar recursos
        
        logger.critical(f"‚ùå Falha fatal ao conectar Supabase ap√≥s retries.")
        return None

    # --- M√âTODOS DE NEG√ìCIO ---

    def get_active_subscribers(self):
        if not self.client: return []
        try:
            return self.client.table('subscribers').select('email, name').eq('is_active', True).execute().data
        except Exception as e:
            logger.error(f"Erro DB (Subscribers): {e}")
            return []

    def should_send_email(self, subscriber_id, region, risk_data):
        # L√≥gica de verifica√ß√£o de envio (placeholder ou real conforme necessidade)
        return True, "hash_placeholder"

    def log_email_sent(self, subscriber_id, region, risk_hash):
        pass 

    def save_risk_history(self, records):
        if not self.client or not records: return
        try:
            self.client.table("risk_history").insert(records).execute()
            logger.info(f"üíæ Hist√≥rico salvo: {len(records)} registros.")
        except Exception as e:
            logger.error(f"Erro DB (Save Risk): {e}")

    def save_market_metrics(self, metrics):
        """
        Transforma m√©tricas espec√≠ficas em formato padronizado para o banco.
        De: {'basis_risk': 50, ...}
        Para: [{'category': 'BASIS', 'value': 50, ...}, ...]
        """
        if not self.client or not metrics: return

        try:
            records = []
            # Usa o hor√°rio do objeto timezone definido no __init__
            now = datetime.now(self.tz).isoformat() 

            # 1. Mapeamento de BASIS
            if 'basis_risk' in metrics:
                records.append({
                    "category": "BASIS",
                    "value": float(metrics.get('basis_risk', 0)),
                    "status": metrics.get('basis_status', 'Normal'),
                    "created_at": now
                })

            # 2. Mapeamento de FAI (Fertilizantes)
            if 'fertilizer_risk' in metrics:
                # Se fertilizer_risk for string/status, ajusta
                val = metrics.get('fertilizer_risk')
                status_text = metrics.get('fai_status', str(val))
                records.append({
                    "category": "FAI",
                    "value": 0.0, # FAI geralmente √© qualitativo, valor 0 placeholder
                    "status": status_text,
                    "created_at": now
                })

            # 3. Mapeamento de CHINA
            if 'china_demand' in metrics:
                c_data = metrics['china_demand']
                # Se vier como dicion√°rio {'score': 20, 'status': 'NORMAL'}
                if isinstance(c_data, dict):
                    records.append({
                        "category": "CHINA",
                        "value": float(c_data.get('score', 0)),
                        "status": c_data.get('status', 'Normal'),
                        "created_at": now
                    })
                else:
                    records.append({
                        "category": "CHINA",
                        "value": 0.0,
                        "status": str(c_data),
                        "created_at": now
                    })

            # Envia tudo de uma vez se houver registros
            if records:
                self.client.table("market_metrics").insert(records).execute()
                logger.info(f"üìà M√©tricas salvas: {len(records)} registros inseridos.")

        except Exception as e:
            logger.error(f"Erro DB (Metrics Transformation): {e}", exc_info=True)

    def get_already_sent_news_ids(self):
        if not self.client: return []
        try:
            res = self.client.table("sent_news_log").select("news_id").limit(100).execute()
            return [x['news_id'] for x in res.data] if res.data else []
        except Exception:
            return []

    def mark_news_as_sent(self, news_ids):
        pass


============================================================
ARQUIVO: context.py
LOCAL: core/context.py
============================================================

from collections import defaultdict
from core.seasonality import RiskAnalyzer

class RiskContext:
    """
    Respons√°vel por manter o estado da execu√ß√£o atual (In-Memory).
    Acumula scores, detecta clusters e mant√©m trackers de benchmarks.
    """
    def __init__(self):
        self.cluster_scores = defaultdict(list)
        self.cluster_details = defaultdict(list)
        self.critical_clusters = []
        self.pillar_sums = {"Mercado": 0, "Log√≠stica": 0, "Clima": 0, "C√¢mbio": 0}
        self.processed_count = 0
        self.avg_global_score = 0.0
        
        # Trackers Espec√≠ficos
        self.max_washout_risk = {"score": 0, "status": "NORMAL", "location": "Global", "moisture": "N/A"}
        self.logistics_benchmark = {"value": 0, "status": "Normal", "port": "Santos", "display_val": "0.00"}
        self.china_metrics = {"status": "Normal", "margin": "Est√°vel"}

        # M√©tricas de Portf√≥lio
        self.total_exposure_brl = 0.0
        self.weighted_pd_sum = 0.0
        self.exposure_at_critical_risk = 0.0 # VaR (Value at Risk)
        self.contract_count = 0

    def update_metrics(self, loc_name, raw_scores, metrics, climate_context):
        """Atualiza os acumuladores globais e trackers espec√≠ficos."""
        # 1. Acumuladores de Pilares
        for pilar in self.pillar_sums:
            self.pillar_sums[pilar] += raw_scores.get(pilar, 0)
        self.processed_count += 1

        # 2. Tracker Washout
        current_w_score = metrics.get('washout_risk', {}).get('score', 0)
        if current_w_score > self.max_washout_risk['score']:
            self.max_washout_risk = {
                "score": current_w_score,
                "status": metrics['washout_risk']['status'],
                "location": loc_name.replace('_', ' '),
                "moisture": climate_context['status_desc']
            }

        # 3. Tracker Santos (Benchmark)
        if loc_name == 'Porto_Santos':
            val_basis = raw_scores['Log√≠stica']
            self.logistics_benchmark = {
                "value": val_basis,
                "status": metrics.get('basis_status', 'Normal').split(':')[-1].strip(),
                "port": "Santos",
                "display_val": f"{val_basis:.2f}" if val_basis != 0 else "Paridade"
            }

        # 4. Tracker China
        if 'China' in loc_name:
            # Nota: O valor exato da varia√ß√£o vem de fora, mas o status vem das m√©tricas
            self.china_metrics["status"] = metrics.get('china_demand', {}).get('status', 'Normal')

    def register_score(self, cluster_name, loc_name, raw_scores, current_month):
        """Calcula score ponderado e registra no cluster."""
        analyzer = RiskAnalyzer(raw_scores)
        weighted = analyzer.calculate_weighted_risk(target_month=current_month)
        final_score = weighted['score_total']

        self.cluster_scores[cluster_name].append(final_score)
        if final_score > 75:
            self.cluster_details[cluster_name].append(loc_name)
            
        return final_score

    def analyze_systemic_risk(self):
        """Processa os clusters para definir se h√° crise sist√™mica."""
        for cluster_name, scores in self.cluster_scores.items():
            avg_cluster_score = sum(scores) / len(scores)
            critical_members = len(self.cluster_details[cluster_name])
            total_members = len(scores)
            
            is_critical = False
            reason = ""

            if cluster_name == 'GLOBAL_CHOKEPOINTS':
                if critical_members >= 1:
                    is_critical = True
                    reason = f"Gargalo Log√≠stico em {self.cluster_details[cluster_name][0]}"
            else:
                if avg_cluster_score > 70:
                    is_critical = True
                    reason = f"Colapso Regional (M√©dia: {avg_cluster_score:.0f})"
                elif critical_members >= (total_members / 2):
                    is_critical = True
                    reason = f"Falha Sist√™mica ({critical_members}/{total_members} locais)"

            if is_critical:
                self.critical_clusters.append(f"{cluster_name}: {reason}")

        # Score Global
        all_scores = [s for scores in self.cluster_scores.values() for s in scores]
        self.avg_global_score = sum(all_scores) / len(all_scores) if all_scores else 0

    def update_portfolio_metrics(self, pd_score, loan_amount, collateral_status):
        """
        Acumula m√©tricas para vis√£o consolidada de carteira.
        """
        self.contract_count += 1
        self.total_exposure_brl += loan_amount
        self.weighted_pd_sum += (pd_score * loan_amount)
        
        if collateral_status in ['WARNING', 'CRITICAL_UNCOVERED'] or pd_score > 70:
            self.exposure_at_critical_risk += loan_amount

    def get_portfolio_summary(self):
        avg_pd = self.weighted_pd_sum / self.total_exposure_brl if self.total_exposure_brl > 0 else 0
        return {
            "total_exposure": round(self.total_exposure_brl, 2),
            "avg_weighted_pd": round(avg_pd, 2),
            "value_at_risk": round(self.exposure_at_critical_risk, 2),
            "risk_concentration_ratio": round(self.exposure_at_critical_risk / self.total_exposure_brl, 4) if self.total_exposure_brl > 0 else 0
        }


============================================================
ARQUIVO: advisor.py
LOCAL: core/advisor.py
============================================================

# core/advisor.py
import logging

logger = logging.getLogger(__name__)

class RiskAdvisor:
    """
    XAI Engine (Explainable AI) para Risco de Cr√©dito Agro.
    Gera narrativas determin√≠sticas baseadas em gatilhos fundamentais.
    """

    def generate_credit_narrative(self, pd_score, metrics):
        """
        Constr√≥i um laudo t√©cnico detalhado explicando o 'Porqu√™' do score.
        Estrutura: [VEREDITO] + [CAUSA RAIZ CLIM√ÅTICA] + [SA√öDE FINANCEIRA] + [FATOR LOG√çSTICO].
        """
        narrative_parts = []

        # 1. VEREDITO INICIAL (O "Headline")
        if pd_score > 70:
            narrative_parts.append("üî¥ PERFIL CR√çTICO: Probabilidade de Default elevada.")
        elif pd_score > 40:
            narrative_parts.append("üü° PERFIL ALERTA: Sinais de deteriora√ß√£o da capacidade de pagamento.")
        else:
            narrative_parts.append("üü¢ PERFIL ROBUSTO: Opera√ß√£o dentro dos par√¢metros de seguran√ßa.")

        # 2. AN√ÅLISE CLIM√ÅTICA (A Causa Raiz Biol√≥gica)
        # Extrai a perda de produtividade calculada no engine
        yield_loss_str = metrics.get('yield_loss_est', '0%')
        yield_loss_val = float(yield_loss_str.strip('%'))
        
        if yield_loss_val > 15.0:
            narrative_parts.append(f"Quebra de safra severa estimada em {yield_loss_str} devido a estresse t√©rmico/h√≠drico na janela cr√≠tica.")
        elif yield_loss_val > 5.0:
            narrative_parts.append(f"Perda marginal de produtividade ({yield_loss_str}) detectada, pressionando levemente o fluxo de caixa.")
        else:
            narrative_parts.append("Condi√ß√µes clim√°ticas favor√°veis sustentam a produtividade projetada.")

        # 3. AN√ÅLISE FINANCEIRA (LTV e Garantias)
        ltv = metrics.get('ltv', 0)
        collateral_val = metrics.get('collateral_value_brl', 0)
        
        if ltv > 1.0:
            narrative_parts.append(f"‚ö†Ô∏è ESTRUTURA DESCOBERTA: LTV projetado de {ltv:.2f}x indica insufici√™ncia de garantias (Colateral: R$ {collateral_val:,.0f}).")
        elif ltv > 0.7:
            narrative_parts.append(f"Alavancagem moderada (LTV {ltv:.2f}x), exigindo monitoramento da liquidez.")
        else:
            narrative_parts.append(f"Excelente cobertura de garantias (LTV {ltv:.2f}x), mitigando risco de perda final (LGD).")

        # 4. AN√ÅLISE LOG√çSTICA (O "Custo Brasil")
        # Se o pre√ßo do frete/basis estiver estressado
        basis_status = metrics.get('basis_status', 'Normal')
        if "Estressado" in basis_status:
            narrative_parts.append("Log√≠stica pressionada: Custo de escoamento corr√≥i a margem l√≠quida do produtor.")
        
        # 5. AN√ÅLISE COMPORTAMENTAL (Serasa/D√≠vida)
        # Recuperamos isso indiretamente se o PD for alto mas o clima for bom
        if pd_score > 50 and yield_loss_val < 5.0:
            narrative_parts.append("Risco impulsionado majoritariamente por fatores comportamentais (Score de Cr√©dito/Endividamento pr√©vio).")

        # Montagem Final
        full_narrative = " ".join(narrative_parts)
        return full_narrative


============================================================
ARQUIVO: market_data.py
LOCAL: core/market_data.py
============================================================

# ARQUIVO: core/market_data.py
import pandas as pd
import time
from core.db import DatabaseManager
from core.logger import get_logger

# Configura log
logger = get_logger(__name__)

class MarketDataError(Exception):
    """Exce√ß√£o customizada para falhas cr√≠ticas na camada de dados de mercado."""
    pass

class MarketLoader:
    @staticmethod
    def get_market_data(tickers: list, period="6mo"):
        """
        Busca dados BLINDADOS do Supabase.
        Implementa l√≥gica de Retry para erros de 'Device busy' e valida√ß√£o rigorosa.
        """
        if not tickers:
            logger.error("Tentativa de buscar dados com lista de tickers vazia.")
            raise MarketDataError("A lista de tickers fornecida est√° vazia.")

        logger.info(f"Buscando dados no DB para {len(tickers)} ativos (Per√≠odo: {period})")
        
        # Instancia o DatabaseManager
        db = DatabaseManager(use_service_role=False)
        
        # --- CONFIGURA√á√ÉO DO RETRY (BLINDAGEM) ---
        max_retries = 5
        backoff_factor = 2 # Segundos iniciais de espera

        for attempt in range(max_retries):
            try:
                # Tenta executar a query
                # Nota: O limite de 2000 linhas √© um buffer de seguran√ßa
                response = db.client.table("market_prices")\
                    .select("ticker, close, date")\
                    .in_("ticker", tickers)\
                    .order("date", desc=True)\
                    .limit(2000)\
                    .execute()
                
                data = response.data
                
                # VALIDA√á√ÉO 1: Banco vazio
                if not data:
                    msg = "Cache do Supabase est√° vazio. O Worker de mercado rodou com sucesso?"
                    logger.warning(msg)
                    # Se n√£o tem dados, n√£o adianta tentar de novo, √© erro de l√≥gica/worker
                    raise MarketDataError(msg)

                # Processamento (Pivot)
                df_db = pd.DataFrame(data)
                df_pivot = df_db.pivot(index='date', columns='ticker', values='close')
                
                # Valida√ß√£o de tickers ausentes (Transformamos em INFO em vez de WARNING se for esperado)
                missing_tickers = set(tickers) - set(df_pivot.columns)
                if missing_tickers:
                    logger.info(f"‚ÑπÔ∏è Tickers ausentes no cache (ignorado no c√°lculo): {missing_tickers}")
                
                # Preenchimento de lacunas (Forward Fill)
                df_pivot = df_pivot.ffill()
                
                # Se um ticker essencial (como USDBRL=X) estiver faltando, a√≠ sim lan√ßamos erro
                if "USDBRL=X" not in df_pivot.columns:
                    raise MarketDataError("Ativo cr√≠tico (D√≥lar) ausente no banco de dados.")

                # VALIDA√á√ÉO 2: DataFrame vazio p√≥s-processamento
                if df_pivot.empty:
                    raise MarketDataError("DataFrame resultante do pivot est√° vazio ap√≥s processamento.")
                
                logger.info(f"‚úÖ Dados carregados com sucesso. Shape: {df_pivot.shape}")
                
                # LOG DE AUDITORIA: Indica que os dados est√£o vindo de uma fonte sandbox
                logger.warning("‚ö†Ô∏è DATA SOURCE: Using SANDBOX (Yahoo Finance) for PoC purposes. Latency: ~15min.")
                
                return df_pivot

            except MarketDataError as e:
                # Se o erro for de l√≥gica (ex: dados vazios), n√£o faz retry, falha logo.
                raise e

            except Exception as e:
                error_msg = str(e)
                # Verifica se √© o erro de recurso ocupado ou conex√£o
                if "Device or resource busy" in error_msg or "ConnectError" in error_msg:
                    # Se ainda tiver tentativas, espera e tenta de novo
                    if attempt < max_retries - 1:
                        wait_time = backoff_factor * (attempt + 1)
                        logger.warning(f"‚è≥ OS Busy/Erro de Conex√£o (Tentativa {attempt+1}/{max_retries}). Esperando {wait_time}s... Erro: {error_msg}")
                        time.sleep(wait_time)
                        continue # Vai para a pr√≥xima itera√ß√£o do loop
                
                # Se esgotou as tentativas ou √© um erro desconhecido cr√≠tico
                logger.error(f"‚ùå Falha cr√≠tica ap√≥s {attempt+1} tentativas: {error_msg}", exc_info=True)
                raise MarketDataError(f"Falha de infraestrutura no Supabase: {error_msg}")


============================================================
ARQUIVO: seasonality.py
LOCAL: core/seasonality.py
============================================================

class SeasonalityManager:
    def __init__(self):
        # MATRIZ DE SENSIBILIDADE FENOL√ìGICA POR ESTADO (UF)
        # Pesos calibrados conforme o calend√°rio m√©dio de cada estado
        self.state_weights = {
            "MT": { # Mato Grosso: Plantio precoce
                9: 0.6, 10: 1.0, 11: 1.5, 12: 2.0, 1: 1.8, 2: 0.8
            },
            "PR": { # Paran√°: Ciclo ligeiramente deslocado
                10: 0.6, 11: 1.2, 12: 1.8, 1: 2.0, 2: 1.5, 3: 0.8
            },
            "GO": { # Goi√°s: Similar ao MT, mas com janelas variadas
                10: 0.7, 11: 1.3, 12: 1.9, 1: 2.0, 2: 1.2, 3: 0.7
            },
            "RS": { # Rio Grande do Sul: Ciclo tardio, alto risco em Jan/Fev
                10: 0.5, 11: 0.8, 12: 1.5, 1: 2.0, 2: 2.0, 3: 1.5, 4: 0.8
            },
            "MS": { # Mato Grosso do Sul
                9: 0.5, 10: 1.0, 11: 1.4, 12: 1.9, 1: 2.0, 2: 1.0
            },
            "BA": { # Bahia (MATOPIBA): Ciclo mais tardio
                11: 0.6, 12: 1.0, 1: 1.5, 2: 2.0, 3: 1.8, 4: 1.0
            }
        }
        self.default_weight = 1.0

    def get_state_weight(self, month, state_code):
        """
        Retorna o peso de sensibilidade fenol√≥gica exato para o Estado (UF).
        """
        state_data = self.state_weights.get(state_code.upper())
        if state_data:
            return state_data.get(month, self.default_weight)
        return self.default_weight

    def get_weight(self, month, region_group):
        if region_group == 'GLOBAL': return 1.0
        if month in self.weights:
            return self.weights[month].get(region_group, self.default_weight)
        return self.default_weight

class RiskAnalyzer:
    def __init__(self, raw_scores):
        self.scores = raw_scores
        self.manager = SeasonalityManager()

    def calculate_weighted_risk(self, target_month):
        # Passo 1: Definir o peso sazonal
        weight = self.manager.get_weight(target_month, 'BR')
        
        # Passo 2: Separar os componentes
        s_clima = self.scores.get('Clima', 0)
        s_log = self.scores.get('Log√≠stica', 0)
        s_mkt = self.scores.get('Mercado', 0)
        s_cambio = self.scores.get('C√¢mbio', 0)
        
        # Passo 3: C√°lculo Inteligente (NOVA F√ìRMULA)
        # Sazonalidade afeta CLIMA e LOG√çSTICA. Mercado/C√¢mbio t√™m peso 1.0 fixo.
        weighted_clima = s_clima * weight
        weighted_log = s_log * weight 
        
        numerator = (weighted_clima + weighted_log + s_mkt + s_cambio)
        denominator = (weight + weight + 1.0 + 1.0) # Soma dos pesos
        
        if denominator == 0: denominator = 1
            
        final_score = numerator / denominator
        
        return {
            'score_total': min(final_score, 100),
            'seasonality_weight': weight
        }


============================================================
ARQUIVO: data_loader.py
LOCAL: core/data_loader.py
============================================================

import datetime
import pytz
import yfinance as yf
import pandas as pd

def fetch_market_data(
    tickers: list[str],
    rename_map: dict,
    lookback_days: int = 365,
    timezone: str = "UTC"
) -> pd.DataFrame:

    tz = pytz.timezone(timezone)
    end_date = datetime.datetime.now(tz)
    start_date = end_date - datetime.timedelta(days=lookback_days)

    data = yf.download(
        tickers,
        start=start_date,
        end=end_date,
        progress=False
    )["Close"]

    if rename_map:
        data = data.rename(columns=rename_map)

    data = data.ffill()

    if data.empty:
        raise RuntimeError("Nenhum dado retornado do Yahoo Finance.")

    return data



============================================================
ARQUIVO: validation_engine.py
LOCAL: core/validation_engine.py
============================================================

import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from core.db import DatabaseManager
from core.logger import get_logger

logger = get_logger("ValidationEngine")

class IBGEValidationEngine:
    def __init__(self, db_manager):
        self.db = db_manager

    def run_accuracy_test(self, simulation_tag: str):
        """
        Confronta a previs√£o do Modelo (Backtest) com a Realidade (CONAB/IBGE).
        """
        logger.info(f"‚öñÔ∏è Iniciando Auditoria de Acur√°cia | Tag: {simulation_tag}")

        # 1. Busca Resultados do Modelo (O que o rob√¥ previu)
        sim_id = self._get_sim_id(simulation_tag)
        if not sim_id:
            logger.error("Simula√ß√£o n√£o encontrada.")
            return {}

        # Pega PD Score e Estado do contrato
        # Precisamos fazer um JOIN com a tabela de portfolio para saber o estado
        res_model = self.db.client.table("backtest_results")\
            .select("contract_id, pd_score, credit_portfolio(state_code)")\
            .eq("simulation_id", sim_id)\
            .execute()
        
        if not res_model.data:
            logger.error("Sem resultados de backtest para analisar.")
            return {}

        # Normaliza dados do modelo
        data_model = []
        for row in res_model.data:
            state = row['credit_portfolio']['state_code']
            pd_score = row['pd_score']
            data_model.append({'state': state, 'predicted_risk': pd_score})
        
        df_model = pd.DataFrame(data_model)
        # M√©dia de risco previsto por estado
        df_model_agg = df_model.groupby('state')['predicted_risk'].mean().reset_index()

        # 2. Busca a Verdade Terrestre (O que aconteceu na CONAB)
        res_truth = self.db.client.table("official_crop_stats")\
            .select("state_code, yield_kg_ha")\
            .eq("crop_year", "2023/2024")\
            .execute()
        
        df_truth = pd.DataFrame(res_truth.data)
        
        # 3. Cruzamento (Merge)
        df_final = pd.merge(df_model_agg, df_truth, left_on='state', right_on='state_code')
        
        if df_final.empty:
            logger.warning("N√£o foi poss√≠vel cruzar estados do modelo com dados da CONAB.")
            return {}

        return self._calculate_metrics(df_final)

    def _calculate_metrics(self, df):
        """
        Calcula a correla√ß√£o entre Risco Previsto (PD) e Quebra Real (Yield Inverso).
        Teoria: Quanto MENOR a produtividade (Yield), MAIOR deve ser o PD.
        Logo, a correla√ß√£o deve ser NEGATIVA forte.
        """
        # Correla√ß√£o de Pearson
        corr, p_value = pearsonr(df['predicted_risk'], df['yield_kg_ha'])
        
        # Erro M√©dio (apenas ilustrativo, pois as escalas s√£o diferentes)
        # O importante aqui √© a Dire√ß√£o da Correla√ß√£o.
        
        metrics = {
            "correlation": corr, # Esperado: Negativo (ex: -0.80)
            "p_value": p_value,
            "sample_size": len(df),
            "details_by_state": df.to_dict(orient='records')
        }
        
        logger.info(f"üìä Resultado da Auditoria: Correla√ß√£o {corr:.2f} (P-Value: {p_value:.4f})")
        return metrics

    def _get_sim_id(self, tag):
        res = self.db.client.table("backtest_simulations").select("id").eq("simulation_name", tag).execute()
        return res.data[0]['id'] if res.data else None


============================================================
ARQUIVO: logger.py
LOCAL: core/logger.py
============================================================

# ARQUIVO: core/logger.py
import logging
import json
import sys
from datetime import datetime

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_record = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "agro-risk-engine",
            "module": record.module,
            "function": record.funcName,
            "message": record.getMessage(),
        }
        if record.exc_info:
            log_record["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_record)

def get_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(JsonFormatter())
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger


============================================================
ARQUIVO: engine.py
LOCAL: core/engine.py
============================================================

import pandas as pd
import numpy as np
from datetime import datetime
from core.indicators.technical import TechnicalIndicators as Tech
from core.indicators.fundamental import FundamentalIndicators as Fund
from core.indicators.macro import MacroIndicators as Macro
from core.seasonality import SeasonalityManager # Importa a intelig√™ncia existente
import logging

logger = logging.getLogger(__name__)

class RiskEngine:
    def __init__(self):
        # Inicializa o gestor de sazonalidade por Estado
        from core.seasonality import SeasonalityManager
        self.seasonality = SeasonalityManager() # Instancia o gestor de safras

    def _sanitize_metrics(self, data):
        if isinstance(data, dict): return {k: self._sanitize_metrics(v) for k, v in data.items()}
        elif isinstance(data, float):
            return 0.0 if np.isnan(data) or np.isinf(data) else round(data, 4)
        return data

    def _is_data_stale(self, df: pd.DataFrame) -> bool:
        if df.empty: return True
        if len(df) >= 3:
            if df['ZS=F'].tail(3).std() == 0 or df['USDBRL=X'].tail(3).std() == 0:
                return True
        last_update = pd.to_datetime(df.index[-1])
        return (datetime.now() - last_update.replace(tzinfo=None)).days > 2

    def _calculate_calibrated_market_score(self, soy_series, usd_series):
        """
        [CORRIGIDO] L√≥gica calibrada: Queda de pre√ßo n√£o √© Risco 80.
        """
        # 1. Volatilidade
        soy_vol = Tech.calculate_volatility(soy_series)
        vol_score = min((soy_vol * 100) * 3, 100)
        
        # 2. Tend√™ncia (CORRIGIDO AQUI)
        trend = Tech.analyze_trend(soy_series)
        # Reduzimos drasticamente as pontua√ß√µes. Queda de pre√ßo √© normal.
        if trend == "STRONG_DOWN": price_risk = 40 # Antes era 80
        elif trend == "DOWN": price_risk = 25      # Antes era 60
        elif trend == "NEUTRAL": price_risk = 10   # Antes era 40
        else: price_risk = 5 
        
        # 3. C√¢mbio
        usd_trend = Tech.analyze_trend(usd_series)
        fx_hedge = -20 if "UP" in usd_trend else 0
        
        final_score = (vol_score * 0.3) + (price_risk * 0.7) + fx_hedge
        return max(0, min(final_score, 100))

    def calculate_full_analysis(self, df_market, loc_name, df_climate=None, month=None):
        # Verifica√ß√£o de integridade de colunas
        if 'ZS=F' not in df_market.columns or 'USDBRL=X' not in df_market.columns:
            logger.warning("‚ö†Ô∏è Dados de mercado incompletos. Retornando an√°lise neutra.")
            return self._get_empty_analysis()  # Retorna scores neutros em vez de quebrar

        stale = self._is_data_stale(df_market)
        soy = df_market['ZS=F']
        usd = df_market['USDBRL=X']
        
        trend = Tech.analyze_trend(soy) 
        geo = Macro.calculate_geopolitical_risk(df_market.get('GC=F', soy), df_market.get('CL=F', soy))
        stress = Macro.calculate_currency_stress(usd)
        
        raw_basis = Fund.calculate_basis_proxy(
            Tech.calculate_volatility(soy), 
            usd.pct_change().iloc[-1], 
            "NEUTRO", 
            stale,
            trend 
        )

        seasonality_factor = 0.8 if month in [3, 4] else 0.2
        score_logistica = min(100, raw_basis * (1 + (seasonality_factor ** 2)))

        china = Fund.calculate_china_demand(soy, df_market.get('HE=F'))
        
        climate_score = 10 
        climate_lvl = "NORMAL"
        
        if df_climate is not None and not df_climate.empty:
            loc_climate = df_climate[df_climate['Location'] == loc_name]
            if not loc_climate.empty:
                climate_score = float(loc_climate.iloc[0]['Risk_Score'])
                climate_lvl = loc_climate.iloc[0]['Risk_Status']

        # Usa a nova fun√ß√£o calibrada
        market_score = self._calculate_calibrated_market_score(soy, usd)
        
        washout = Fund.calculate_washout_probability(climate_lvl, trend, soy.pct_change(30).iloc[-1])
        
        results = {
            "Mercado": market_score,
            "C√¢mbio": min(100, (Tech.calculate_rsi(usd)*0.5) + (stress['score']*0.5)),
            "Log√≠stica": score_logistica,
            "Clima": climate_score 
        }
        
        metrics = {
            "washout_risk": washout,
            "china_demand": china,
            "geopolitics": geo,
            "is_stale": stale,
            "basis_status": f"Basis {loc_name}: {'Estressado' if score_logistica > 60 else 'Normal'}"
        }
        
        return self._sanitize_metrics(results), self._sanitize_metrics(metrics)
    
    class OpportunityEngine:
        @staticmethod
        def analyze_profit_windows(df_market, fai_status):
            opportunities = []
            if "OPORTUNIDADE" in str(fai_status).upper():
                opportunities.append("üí∞ BARTER: Rela√ß√£o de troca favor√°vel.")
            soy_rsi = Tech.calculate_rsi(df_market['ZS=F'])
            if soy_rsi < 30:
                opportunities.append(f"üìà T√âCNICO: Soja em sobrevenda (RSI {soy_rsi:.0f}).")
            usd_rsi = Tech.calculate_rsi(df_market['USDBRL=X'])
            if usd_rsi < 30:
                opportunities.append("üö¢ LOG√çSTICA: D√≥lar em baixa.")
            return opportunities

    def calculate_market_score(self, ticker: str, df_market: pd.DataFrame) -> float:
        """
        Calculate the market risk score for a given ticker.
        """
        try:
            if ticker not in df_market.columns:
                raise ValueError(f"Ticker {ticker} not found in market data.")

            # Calculate volatility
            vol_30d = df_market[ticker].pct_change().rolling(window=30).std() * np.sqrt(252) * 100
            current_vol = vol_30d.iloc[-1]

            # Soybean-specific logic (e.g., Crush Margin)
            if ticker in ["ZS=F", "ZM=F", "ZL=F"]:  # Soybean, Soybean Meal, Soybean Oil
                crush_margin = self._calculate_crush_margin(df_market)
                logger.info(f"Crush Margin for {ticker}: {crush_margin:.2f}")
                # Adjust score based on crush margin
                if crush_margin < 50:
                    return min(current_vol * 1.2, 100)  # Higher risk if margin is low
                else:
                    return min(current_vol, 100)

            # General logic for other commodities (e.g., Wheat, Corn)
            logger.info(f"Volatility for {ticker}: {current_vol:.2f}")
            if current_vol > 40:  # High volatility threshold
                return 100.0
            elif current_vol > 20:  # Moderate volatility
                return 50.0
            else:
                return 20.0  # Low volatility

        except Exception as e:
            logger.error(f"Error calculating market score for {ticker}: {e}")
            return 0.0

    def _calculate_crush_margin(self, df_market: pd.DataFrame) -> float:
        """
        Calculate the soybean crush margin based on Soybean, Soybean Meal, and Soybean Oil prices.
        """
        try:
            if not all(t in df_market.columns for t in ["ZS=F", "ZM=F", "ZL=F"]):
                raise ValueError("Missing data for crush margin calculation.")

            # Example formula: Crush Margin = (Meal + Oil) - Soybean
            meal_price = df_market["ZM=F"].iloc[-1]
            oil_price = df_market["ZL=F"].iloc[-1]
            soybean_price = df_market["ZS=F"].iloc[-1]
            crush_margin = (meal_price + oil_price) - soybean_price
            return crush_margin

        except Exception as e:
            logger.error(f"Error calculating crush margin: {e}")
            return 0.0

    def calculate_pd_metrics(self, df_market, loc_name, df_climate, contract_data, month):
        """
        Calcula a Probabilidade de Default (PD) com Regra de Veto Clim√°tico.
        """
        # 1. Risco Produtivo (Safra)
        raw_scores, metrics = self.calculate_full_analysis(df_market, loc_name, df_climate, month)
        
        # Pesos Base: Clima (40%) + Log√≠stica (30%) + Mercado (20%) + C√¢mbio (10%)
        productive_pd = (
            (raw_scores.get('Clima', 0) * 0.4) +
            (raw_scores.get('Log√≠stica', 0) * 0.3) +
            (raw_scores.get('Mercado', 0) * 0.2) +
            (raw_scores.get('C√¢mbio', 0) * 0.1)
        )
        
        # 2. Risco Comportamental
        serasa = contract_data.get('credit_score_serasa', 700)
        dti = contract_data.get('debt_to_income_ratio', 0.3)
        
        # Normaliza Serasa (0 a 1000 -> Risco 100 a 0)
        behavioral_risk = (1000 - serasa) / 10 
        # Penaliza DTI alto (se DTI > 0.5, adiciona risco exponencial)
        if dti > 0.5:
            behavioral_risk += (dti * 40) # Penalidade agressiva por alavancagem
        
        # 3. PD FINAL (H√≠brida com GATILHO DE CATA√ÅSTROFE)
        
        climate_score = raw_scores.get('Clima', 0)
        
        # --- AQUI EST√Å A MUDAN√áA (REGRA DE VETO) ---
        if climate_score > 80:
            # Se o clima destruiu a safra, o comportamento importa pouco (10%)
            # O risco produtivo domina (90%) e ganha um boost de severidade
            final_pd = (productive_pd * 0.9) + (behavioral_risk * 0.1)
            final_pd = final_pd * 1.2 # Boost de P√¢nico
        elif climate_score > 50:
            # Situa√ß√£o de Alerta
            final_pd = (productive_pd * 0.7) + (behavioral_risk * 0.3)
        else:
            # Situa√ß√£o Normal (Comportamento do cliente pesa mais)
            final_pd = (productive_pd * 0.4) + (behavioral_risk * 0.6)
            
        # Teto de 99.9%
        final_pd = min(final_pd, 99.9)
        
        # --- FIM DA MUDAN√áA ---

        # C√°lculo de LTV Estressado
        current_price_brl = metrics.get('market_price_brl', 120.0)
        
        credit_metrics = self._calculate_ltv_exposure(
            contract_data, 
            climate_score, # Passa o score clim√°tico puro para o LTV
            current_price_brl,
            month
        )
        
        metrics.update(credit_metrics)
        return round(final_pd, 2), metrics

    def _calculate_dynamic_lgd(self, exposure, collateral_value):
        """
        Calcula a Loss Given Default baseada na cobertura de garantia.
        Padr√£o Institucional: Considera custos de execu√ß√£o (Haircut).
        """
        if exposure <= 0: return 0.0
        
        # Haircut de Execu√ß√£o (Advogados, Leil√£o, Liquidez da Terra)
        # Terras no MT t√™m liquidez alta (Haircut 20%)
        # Terras em fronteira agr√≠cola t√™m liquidez baixa (Haircut 40%)
        execution_haircut = 0.25 # Conservador (25%)
        
        recoverable_amount = collateral_value * (1 - execution_haircut)
        
        # Se a garantia cobre tudo, LGD √© zero (ou um piso t√©cnico de 5%)
        if recoverable_amount >= exposure:
            return 0.05 # Piso t√©cnico operacional
            
        # C√°lculo da perda residual
        loss = exposure - recoverable_amount
        lgd = loss / exposure
        
        return min(lgd, 1.0) # Teto de 100%

    def _calculate_ltv_exposure(self, contract, climate_score, current_price_brl, month):
        """
        C√°lculo de LTV com Sensibilidade Fenol√≥gica por Estado.
        """
        loan_amount = float(contract.get('loan_amount', 0))
        area = float(contract.get('area_hectares', 0))
        initial_yield = float(contract.get('estimated_yield_kg_ha', 3600))
        state_code = contract.get('state_code', 'MT')

        if loan_amount <= 0 or area <= 0:
            return {"ltv": 0, "collateral_status": "DATA_MISSING"}

        # Recupera o peso fenol√≥gico espec√≠fico do Estado e M√™s
        pheno_weight = self.seasonality.get_state_weight(month, state_code)
        
        # Impacto clim√°tico calibrado: (Score * Sensibilidade Base) * Peso do Estado/M√™s
        yield_reduction_factor = (climate_score * 0.005) * pheno_weight
        stressed_yield = initial_yield * (1 - yield_reduction_factor)
        
        # Valor da Garantia (Safra Estressada * √Årea * Pre√ßo Saca)
        total_collateral_value = (stressed_yield * area) * (current_price_brl / 60)
        
        ltv = (loan_amount / total_collateral_value) if total_collateral_value > 0 else 999
        
        # Status Institucional de Garantia
        status = "HEALTHY"
        if ltv > 0.85: status = "WARNING"
        if ltv > 1.0: status = "CRITICAL_UNCOVERED"

        return {
            "ltv": round(ltv, 4),
            "collateral_value_brl": round(total_collateral_value, 2),
            "collateral_status": status,
            "yield_loss_est": f"{yield_reduction_factor:.1%}",
            "pheno_weight_applied": pheno_weight
        }
    
    def _get_empty_analysis(self):
        """Retorna uma an√°lise neutra para evitar falhas."""
        results = {
            "Mercado": 0.0,
            "C√¢mbio": 0.0,
            "Log√≠stica": 0.0,
            "Clima": 0.0
        }
        metrics = {
            "washout_risk": 0.0,
            "china_demand": 0.0,
            "geopolitics": 0.0,
            "is_stale": True,
            "basis_status": "N/A"
        }
        return results, metrics


============================================================
ARQUIVO: pipeline.py
LOCAL: core/pipeline.py
============================================================

import pytz
import pandas as pd
import numpy as np
from datetime import datetime
import os

# Core Imports
from core.env import load_config
from core.market_data import MarketLoader
from core.engine import RiskEngine
from core.db import DatabaseManager
from core.climate_risk import ClimateIntelligence
from core.scout import NewsScout
from core.logger import get_logger
from core.advisor import RiskAdvisor # 1. Certifique-se de que o import existe

# Componentes Refatorados
from core.context import RiskContext
from core.persister import RiskPersister
from core.factory import RegionalEngineFactory

# INSTANCIA√á√ÉO GLOBAL DO LOGGER (N√≠vel de M√≥dulo)
logger = get_logger(__name__) 

class RiskPipeline:
    def __init__(self, mode: str):
        self.mode = mode
        self.config = load_config()
        
        # Infraestrutura
        self.db = DatabaseManager(use_service_role=True)
        self.engine = RiskEngine()
        self.climate_intel = ClimateIntelligence()
        self.scout = NewsScout(use_service_role=True)
        
        # 2. INICIALIZA√á√ÉO DO ADVISOR (O que estava faltando)
        self.advisor = RiskAdvisor() 
        
        # Componentes de Apoio
        self.context = RiskContext()
        self.persister = RiskPersister(self.db)
        
        self.br_tz = pytz.timezone('America/Sao_Paulo')
        self.now_br = datetime.now(self.br_tz)
        
        self.macro_corr = 0.0 
        # Puxa o mapa de tickers diretamente da config j√° carregada
        self.ticker_map = self.config.get("ticker_map", {})

    def run(self):
        logger.info(f"üöÄ Iniciando Credit Risk Pipeline")
        
        response = self.db.client.table("credit_portfolio").select("*").execute()
        self.contracts = response.data or []
        
        if not self.contracts:
            logger.warning("‚ö†Ô∏è Nenhum contrato encontrado.")
            return

        if not self._load_data(): return

        # Chama a vers√£o inteligente do c√°lculo de correla√ß√£o
        self.macro_corr = self._calculate_macro_correlation()

        self._process_contracts()  # M√©todo renomeado

        # Notifica√ß√£o de Resumo (Morning Call) apenas para o Admin
        if self.mode == "morning":
            admin_email = os.getenv("EMAIL_TO")
            self.notifier.check_and_send(self.mode, self.context, self.df_market, recipient_email=admin_email)

        logger.info("‚úÖ Pipeline finalizado com sucesso.")

    def _calculate_macro_correlation(self) -> float:
        """Calcula correla√ß√£o entre a commodity principal da carteira e o D√≥lar."""
        try:
            if self.df_market is None or "USDBRL=X" not in self.df_market.columns:
                return 0.0

            # Pega a cultura do primeiro contrato como refer√™ncia da carteira
            first_contract = self.contracts[0]
            culture = (first_contract.get("culture") or first_contract.get("commodity", "")).lower()
            primary_ticker = self.ticker_map.get(culture, "ZS=F")

            if primary_ticker not in self.df_market.columns:
                primary_ticker = "ZS=F"

            # Correla√ß√£o de 30 dias (Retornos percentuais s√£o mais precisos que pre√ßos brutos)
            df_recent = self.df_market.tail(30).pct_change()
            correlation = df_recent[primary_ticker].corr(df_recent["USDBRL=X"])

            logger.info(f"üìà Correla√ß√£o Macro ({primary_ticker}/USD): {correlation:.2f}")
            return float(correlation) if not np.isnan(correlation) else 0.0
        except Exception as e:
            logger.error(f"Erro no c√°lculo macro: {e}")
            return 0.0

    def _calculate_backtest_benchmark(self, ticker: str) -> float:
        try:
            # Garante que o ticker existe no DF, sen√£o usa Soja
            active_ticker = ticker if ticker in self.df_market.columns else 'ZS=F'
            series = self.df_market[active_ticker]
            
            delta = series.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rsi = 100 - (100 / (1 + (gain / loss.replace(0, 0.001))))
            
            vol = series.pct_change().rolling(window=30).std() * np.sqrt(252) * 100
            
            if rsi.iloc[-1] > 70 and vol.iloc[-1] > 40:
                return 100.0
            return 30.0
        except Exception as e:
            logger.error(f"Erro no benchmark: {e}")
            return 30.0

    def _process_contracts(self):  # Renomeado de _process_regions
        current_month = self.now_br.month

        for raw_contract in self.contracts:
            try:
                # 1. Mapeamento de Dados do Contrato
                contract = {
                    "id": raw_contract.get("id"),
                    "name": raw_contract.get("client_name"),
                    "state_code": raw_contract.get("state_code", "MT"), # UF vinda do DB
                    "loan_amount": raw_contract.get("loan_amount", 0),
                    "area_hectares": raw_contract.get("area_hectares", 0),
                    "estimated_yield_kg_ha": raw_contract.get("estimated_yield_kg_ha", 3600), # Default 60 sacas/ha
                    "commodity": (raw_contract.get("culture") or "soja").lower()
                }

                # 2. Execu√ß√£o do Motor de PD/LTV
                strategy = RegionalEngineFactory.get_strategy(raw_contract)
                
                # Pre√ßo atual para o c√°lculo de LTV
                current_price_brl = strategy.get_soy_brl_price(self.df_market)
                
                # C√°lculo de PD (Probability of Default)
                pd_score, metrics = self.engine.calculate_pd_metrics(
                    self.df_market, 
                    contract['name'], 
                    self.df_climate, 
                    contract, 
                    current_month
                )
                
                metrics['market_price_brl'] = current_price_brl

                # 1. Gera a justificativa (Explainability)
                risk_justification = self.advisor.generate_credit_narrative(pd_score, metrics)
                metrics['risk_justification'] = risk_justification

                # 2. Atualiza o Contexto Global de Carteira
                self.context.update_portfolio_metrics(
                    pd_score, 
                    contract['loan_amount'], 
                    metrics.get('collateral_status')
                )

                # 3. Persiste no Banco com a Justificativa
                self.db.client.table("credit_portfolio").update({
                    "last_pd_score": pd_score,
                    "current_ltv": metrics.get('ltv'),
                    "collateral_status": metrics.get('collateral_status'),
                    "risk_justification": risk_justification # <--- NOVA COLUNA
                }).eq("id", contract['id']).execute()

            except Exception as e:
                # Agora o logger estar√° definido aqui
                logger.error(f"‚ùå Erro Cr√≠tico no Contrato {raw_contract.get('id')}: {e}")

        # Salva m√©tricas globais ap√≥s o loop
        self.persister.save_market_metrics(self.df_market, self.context)

    def _load_data(self) -> bool:
        try:
            symbols = self.config.get('tickers', [])
            self.df_market = MarketLoader.get_market_data(symbols)
            dynamic_locations = [{'name': c['client_name'], 'lat': float(c['latitude']), 'lon': float(c['longitude'])} for c in self.contracts]
            self.df_climate = self.climate_intel.run_full_scan(locations=dynamic_locations)
            return True
        except Exception as e:
            logger.critical(f"Falha na ingest√£o: {e}", exc_info=True)
            return False

    def _extract_climate_context(self, loc_name):
        ctx = {"status_desc": "N/A", "rain_7d": 0.0, "temp_max": 0.0}
        if not self.df_climate.empty:
            c_row = self.df_climate[self.df_climate['Location'] == loc_name]
            if not c_row.empty:
                ctx = {"status_desc": str(c_row.iloc[0]['Risk_Status']), "rain_7d": float(c_row.iloc[0]['Rain_7d']), "temp_max": float(c_row.iloc[0].get('Temp_Max', 0.0))}
        return ctx


============================================================
ARQUIVO: env.py
LOCAL: core/env.py
============================================================

import os
import yaml
from dotenv import load_dotenv

# Carrega vari√°veis de ambiente (.env) logo no in√≠cio
load_dotenv()

# ==============================================================================
# 1. GERENCIAMENTO DE ARQUIVOS YAML (Configura√ß√µes do Projeto)
# ==============================================================================
def load_config():
    """
    Carrega a configura√ß√£o unificada do projeto (settings.yaml).
    Suporta sobrescrita por dev.yaml para ambiente de desenvolvimento.
    """
    # Caminho base (sobe um n√≠vel da pasta core)
    base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    config_path = os.path.join(base_path, 'configs', 'settings.yaml')
    
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {config_path}")

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # Sobrescrita de Desenvolvimento (Opcional)
    dev_path = os.path.join(base_path, 'configs', 'dev.yaml')
    if os.path.exists(dev_path):
        # Opcional: print("üîß [CONFIG] Carregando overrides de desenvolvimento...")
        with open(dev_path, 'r') as f:
            dev_config = yaml.safe_load(f)
            if dev_config:
                config.update(dev_config)

    return config

# ==============================================================================
# 2. GERENCIAMENTO DE E-MAIL (Sua L√≥gica Original Preservada)
# ==============================================================================
class EmailEnv:
    def __init__(self):
        # Tenta pegar a chave do Resend (Prioridade Alta)
        self.resend_key = os.getenv('RESEND_API_KEY')
        
        # Pega credenciais SMTP (Gmail/Outlook) para compatibilidade/backup
        self.sender = os.getenv('EMAIL_SENDER')
        self.password = os.getenv('EMAIL_PASSWORD')

def load_email_env():
    """
    Valida e retorna as credenciais de e-mail.
    Garante que temos pelo menos UMA forma de envio configurada.
    """
    env = EmailEnv()
    
    # L√≥gica h√≠brida:
    # 1. Se tiver Resend, √≥timo.
    # 2. Se n√£o tiver Resend, PRECISA ter Sender/Password (SMTP).
    
    if not env.resend_key:
        # Se n√£o tem Resend, verifica se tem SMTP configurado
        if not env.sender or not env.password:
             # S√≥ levanta erro se AMBOS estiverem faltando
             print("‚ö†Ô∏è AVISO: Nenhuma configura√ß√£o de e-mail encontrada (Resend ou SMTP).")
             print("   Verifique se as secrets EMAIL_SENDER e EMAIL_PASSWORD est√£o no .env ou GitHub.")
             # N√£o vamos dar raise ValueError aqui para n√£o travar o '--dry-run',
             # mas o envio falhar√° se tentado.
            
    return env


============================================================
ARQUIVO: factory.py
LOCAL: core/factory.py
============================================================

# core/factory.py
from core.strategies.mt_strategy import MatoGrossoStrategy
from core.strategies.pr_strategy import ParanaStrategy

class RegionalEngineFactory:
    @staticmethod
    def get_strategy(loc_data):
        state = loc_data.get('state_code', 'DEFAULT')
        if state == 'MT':
            return MatoGrossoStrategy()
        elif state == 'PR':
            return ParanaStrategy()
        # Fallback ou outras regi√µes futuras
        return ParanaStrategy() 

# core/pipeline.py (Refatorado)
class RiskPipeline:
    def __init__(self, mode, run_shadow_mode=True):
        self.mode = mode
        self.run_shadow_mode = run_shadow_mode
        self.legacy_engine = RiskEngine() # Motor monol√≠tico antigo
        self.db = DatabaseManager(use_service_role=True)
        # ... (restante do init)

    def _process_regions(self):
        regions = self.config.get('locations', [])
        current_month = self.now_br.month

        for loc in regions:
            # 1. NOVO MOTOR (Strategy Pattern)
            strategy = RegionalEngineFactory.get_strategy(loc)
            
            new_scores = {
                "Log√≠stica": strategy.calculate_logistics_risk(self.df_market, loc),
                "Clima": strategy.calculate_climate_risk(self.df_climate, loc, current_month),
                "Mercado": strategy.calculate_market_risk(self.df_market)
            }
            
            # 2. SHADOW MODE (Compara√ß√£o Silenciosa)
            if self.run_shadow_mode:
                legacy_scores, _ = self.legacy_engine.calculate_full_analysis(
                    self.df_market, loc['name'], self.df_climate, month=current_month
                )
                self._save_shadow_log(loc['name'], legacy_scores, new_scores)

            # 3. SEGUE O FLUXO USANDO O MOTOR NOVO
            # (Mantemos a compatibilidade de interface para o restante do pipeline)
            self.persister.save_region_risk(loc, sum(new_scores.values())/3, new_scores, {})

    def _save_shadow_log(self, loc_name, legacy, current):
        """Salva compara√ß√£o no banco para an√°lise do time de dados"""
        log_data = {
            "location": loc_name,
            "legacy_score": sum(legacy.values())/len(legacy),
            "new_strategy_score": sum(current.values())/len(current),
            "timestamp": datetime.now().isoformat()
        }
        # Inserir em uma tabela 'shadow_scoring_logs'
        self.db.client.table("shadow_scoring_logs").insert(log_data).execute()


============================================================
ARQUIVO: climate_risk.py
LOCAL: core/climate_risk.py
============================================================

# ARQUIVO: core/climate_risk.py
import httpx
import asyncio
import pandas as pd
import logging
import random
import os
from dotenv import load_dotenv
from datetime import datetime

# Carrega vari√°veis de ambiente do .env
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ClimateIntelligence:
    def __init__(self):
        self.base_url = "https://api.open-meteo.com/v1/forecast"
        self.weatherapi_key = os.getenv("WEATHERAPI_KEY") # Chave da API Secund√°ria
        
        # Lista de Regi√µes
        self.regions = [
            {'name': 'Sorriso_MT', 'lat': -12.54, 'lon': -55.72, 'type': 'production', 'hemisphere': 'S'},
            {'name': 'Cascavel_PR', 'lat': -24.95, 'lon': -53.45, 'type': 'production', 'hemisphere': 'S'},
            {'name': 'Rio_Verde_GO', 'lat': -17.79, 'lon': -50.92, 'type': 'production', 'hemisphere': 'S'},
            {'name': 'Des_Moines_IA', 'lat': 41.60, 'lon': -93.60, 'type': 'production', 'hemisphere': 'N'},
            {'name': 'Decatur_IL', 'lat': 39.84, 'lon': -88.95, 'type': 'production', 'hemisphere': 'N'},
            {'name': 'Porto_Santos', 'lat': -23.96, 'lon': -46.33, 'type': 'logistics', 'hemisphere': 'S'},
            {'name': 'Paranagua', 'lat': -25.51, 'lon': -48.50, 'type': 'logistics', 'hemisphere': 'S'},
            {'name': 'Itaqui_MA', 'lat': -2.57, 'lon': -44.36, 'type': 'logistics', 'hemisphere': 'S'},
            {'name': 'Santarem_PA', 'lat': -2.44, 'lon': -54.70, 'type': 'logistics', 'hemisphere': 'S'},
            {'name': 'Itacoatiara_AM', 'lat': -3.14, 'lon': -58.44, 'type': 'logistics', 'hemisphere': 'S'},
            {'name': 'Mississippi_River_St_Louis', 'lat': 38.62, 'lon': -90.19, 'type': 'logistics', 'hemisphere': 'N'},
            {'name': 'Panama_Canal', 'lat': 9.10, 'lon': -79.69, 'type': 'chokepoint', 'hemisphere': 'N'},
            {'name': 'Suez_Canal', 'lat': 30.58, 'lon': 32.27, 'type': 'chokepoint', 'hemisphere': 'N'},
            {'name': 'China_Dalian', 'lat': 38.91, 'lon': 121.60, 'type': 'demand', 'hemisphere': 'N'}
        ]

    def _get_synthetic_fallback(self, region, month):
        """
        PLANO B (FINAL): Se todas as APIs falharem, gera dados baseados na m√©dia hist√≥rica.
        Isso impede que o sistema quebre (Crash).
        """
        logger.warning(f"‚ö†Ô∏è Usando Fallback Sint√©tico para {region['name']}")
        
        is_summer = (region['hemisphere'] == 'S' and month in [12, 1, 2]) or \
                    (region['hemisphere'] == 'N' and month in [6, 7, 8])
        
        # Simula dados normais para n√£o gerar p√¢nico falso
        return {
            'rain_7d': 50.0 if is_summer else 10.0, # Chove mais no ver√£o
            'temp_max': 30.0 if is_summer else 15.0,
            'is_estimated': True # Flag para avisar no relat√≥rio
        }

    async def _fetch_single_forecast(self, client, region, semaphore):
        """
        Busca dados com Sem√°foro (limite de conex√µes) e Retry.
        Prioridade: Open-Meteo -> WeatherAPI -> Sint√©tico.
        """
        async with semaphore: # <--- AQUI EST√Å A M√ÅGICA DO CONTROLE DE TR√ÅFEGO
            max_retries = 3
            
            # --- TENTATIVA 1: OPEN-METEO ---
            for attempt in range(max_retries):
                try:
                    # Adiciona um jitter (atraso aleat√≥rio) para n√£o bater na API ao mesmo tempo
                    await asyncio.sleep(random.uniform(0.1, 0.5))
                    
                    params = {
                        "latitude": region['lat'],
                        "longitude": region['lon'],
                        "daily": ["precipitation_sum", "temperature_2m_max"],
                        "timezone": "auto"
                    }
                    
                    response = await client.get(self.base_url, params=params)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if 'daily' in data:
                            rain_7d = sum(data['daily']['precipitation_sum'])
                            temp_max = sum(data['daily']['temperature_2m_max']) / len(data['daily']['temperature_2m_max'])
                            return {'rain_7d': rain_7d, 'temp_max': temp_max, 'is_estimated': False}
                    
                    # Se n√£o for 200, tenta de novo
                    logger.warning(f"API Open-Meteo {region['name']} status {response.status_code}. Tentativa {attempt+1}")

                except Exception as e:
                    # Se for erro de conex√£o (Device busy), espera mais tempo
                    wait = (attempt + 1) * 2
                    logger.warning(f"Erro Conex√£o Open-Meteo ({region['name']}): {e}. Esperando {wait}s...")
                    await asyncio.sleep(wait)

            # --- TENTATIVA 2: WEATHERAPI (FALLBACK) ---
            if self.weatherapi_key:
                try:
                    logger.info(f"üîÑ Tentando WeatherAPI para {region['name']}...")
                    wapi_url = "http://api.weatherapi.com/v1/forecast.json"
                    wapi_params = {
                        "key": self.weatherapi_key,
                        "q": f"{region['lat']},{region['lon']}",
                        "days": 7,
                        "aqi": "no",
                        "alerts": "no"
                    }
                    
                    # Usa o mesmo client http async
                    response = await client.get(wapi_url, params=wapi_params)
                    
                    if response.status_code == 200:
                        data = response.json()
                        forecast_days = data.get('forecast', {}).get('forecastday', [])
                        
                        if forecast_days:
                            rain_7d = sum(day['day']['totalprecip_mm'] for day in forecast_days)
                            temp_max = sum(day['day']['maxtemp_c'] for day in forecast_days) / len(forecast_days)
                            return {'rain_7d': rain_7d, 'temp_max': temp_max, 'is_estimated': False}
                    else:
                        logger.error(f"WeatherAPI falhou com status {response.status_code}")

                except Exception as e:
                    logger.error(f"Erro WeatherAPI ({region['name']}): {e}")

            # --- TENTATIVA 3: SINT√âTICO (FINAL) ---
            # Se falhar todas as tentativas (Open-Meteo e WeatherAPI), retorna o Fallback Sint√©tico
            return self._get_synthetic_fallback(region, datetime.now().month)

    def _is_off_season(self, region_type, hemisphere, month):
        if region_type != 'production': return False
        if hemisphere == 'N' and month in [11, 12, 1, 2, 3]: return True
        if hemisphere == 'S' and month in [6, 7, 8]: return True
        return False

    def analyze_risk(self, weather_data, region_type, hemisphere, current_month):
        # Se vier do fallback sint√©tico, adiciona aviso
        is_estimated = weather_data.get('is_estimated', False)
        status_suffix = " (EST)" if is_estimated else ""

        if self._is_off_season(region_type, hemisphere, current_month):
            return f"ENTRESSAFRA{status_suffix}", 0

        rain = weather_data['rain_7d']
        temp = weather_data['temp_max']
        
        if region_type == 'production':
            if rain < 5: 
                if temp > 35: return "SECA EXTREMA", 100  # N√≠vel Ouro: Risco M√°ximo
                elif temp > 32: return "CALOR + SECA", 70
                else: return "SECA LEVE", 40
            elif rain < 15: 
                return "ATEN√á√ÉO", 20
        
        if rain > 180: return "EXCESSO CHUVA", 100

        elif region_type == 'chokepoint':
            if rain < 5: return "BAIXO N√çVEL", 10

        return "NORMAL", 0

    async def run_full_scan_async(self, locations=None):
        """
        Perform a full climate risk scan. If `locations` is provided, it overrides `self.regions`.
        """
        results = []
        current_month = datetime.now().month
        logger.info(f"üåç Iniciando Scan Clim√°tico (Controlado)...")
        
        # Use provided locations or default to self.regions
        regions_to_scan = locations if locations else self.regions
        
        # --- SEM√ÅFORO: Permite apenas 2 requisi√ß√µes simult√¢neas ---
        semaphore = asyncio.Semaphore(2) 
        
        limits = httpx.Limits(max_keepalive_connections=2, max_connections=4)
        
        async with httpx.AsyncClient(limits=limits, timeout=30.0) as client:
            tasks = [self._fetch_single_forecast(client, region, semaphore) for region in regions_to_scan]
            weather_results = await asyncio.gather(*tasks)
            
            for region, data in zip(regions_to_scan, weather_results):
                status, score = self.analyze_risk(
                    data, 
                    region.get('type', 'production'), 
                    region.get('hemisphere', 'S'), 
                    current_month
                )
                
                results.append({
                    'Location': region['name'],
                    'Group': 'BR' if region.get('hemisphere', 'S') == 'S' else ('US' if region.get('hemisphere', 'S') == 'N' and 'China' not in region['name'] else 'GLOBAL'),
                    'Risk_Status': status,
                    'Risk_Score': score,
                    'Rain_7d': data['rain_7d'],
                    'Temp_Max': data['temp_max']
                })
                
        return pd.DataFrame(results)

    def run_full_scan(self, locations=None):
        """
        Synchronous wrapper for `run_full_scan_async`. If `locations` is provided, it overrides `self.regions`.
        """
        return asyncio.run(self.run_full_scan_async(locations=locations))


============================================================
ARQUIVO: brapi_client.py
LOCAL: core/brapi_client.py
============================================================

# ARQUIVO: core/brapi_client.py
import requests
import pandas as pd
import os
from datetime import datetime
from core.logger import get_logger

logger = get_logger("BrapiClient")

class BrapiClient:
    BASE_URL = "https://brapi.dev/api"

    def __init__(self):
        self.token = os.getenv("BRAPI_TOKEN")
        self.session = requests.Session()

    def get_historical_data(self, ticker: str, range_str="3mo", interval="1d") -> pd.DataFrame:
        """
        Busca dados hist√≥ricos (OHLC) formatados igual ao yfinance.
        ticker: 'USDBRL', 'RAIL3', 'PETR4'
        """
        if not self.token:
            logger.error("Token Brapi n√£o configurado.")
            return pd.DataFrame()

        try:
            # Endpoint de Quote com range hist√≥rico
            url = f"{self.BASE_URL}/quote/{ticker}"
            params = {
                'token': self.token,
                'range': range_str,
                'interval': interval,
                'fundamental': 'false'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            if 'results' not in data or not data['results']:
                return pd.DataFrame()

            # Extrai a s√©rie hist√≥rica
            historical = data['results'][0].get('historicalDataPrice', [])
            if not historical:
                return pd.DataFrame()

            # Transforma em DataFrame Pandas
            df = pd.DataFrame(historical)
            
            # Padroniza colunas para o formato do seu sistema (igual yfinance)
            # Brapi retorna: date, open, high, low, close, volume
            df = df.rename(columns={
                'date': 'Date',
                'open': 'Open',
                'high': 'High',
                'low': 'Low',
                'close': 'Close',
                'volume': 'Volume'
            })
            
            # Converte data (timestamp unix ou string) para datetime
            # A Brapi geralmente manda timestamp. Ajuste conforme retorno.
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'], unit='s') 
            
            df.set_index('Date', inplace=True)
            return df

        except Exception as e:
            logger.error(f"Erro Brapi hist√≥rico para {ticker}: {e}")
            return pd.DataFrame()


============================================================
ARQUIVO: backtest_engine.py
LOCAL: core/backtest_engine.py
============================================================

# core/backtest_engine.py
import pandas as pd
import numpy as np
import logging
from datetime import timedelta
from core.logger import get_logger
from core.climate_risk import ClimateIntelligence
from core.advisor import RiskAdvisor  # <--- IMPORT NOVO

logging.getLogger('InstitutionalBacktest').setLevel(logging.INFO)
logger = get_logger("InstitutionalBacktest")

class InstitutionalBacktestEngine:
    def __init__(self, risk_engine, db_manager):
        self.engine = risk_engine
        self.db = db_manager
        self.climate_intel = ClimateIntelligence()
        self.advisor = RiskAdvisor() # <--- INICIALIZA√á√ÉO DO NARRADOR

    def run_walk_forward(self, simulation_name, start_date, end_date, contracts):
        logger.info(f"üöÄ Iniciando Backtest Institucional: {simulation_name}")
        
        warm_up_start = start_date - timedelta(days=45)
        full_market = self._load_historical_market(warm_up_start, end_date)
        if full_market is None or full_market.empty:
            raise ValueError("‚ùå Falha cr√≠tica: Dados de mercado insuficientes.")

        climate_map = self._load_historical_climate_map(contracts, start_date, end_date)

        # Gest√£o da Simula√ß√£o (Limpeza e Cria√ß√£o)
        existing = self.db.client.table("backtest_simulations").select("id").eq("simulation_name", simulation_name).execute()
        if existing.data:
            sim_id = existing.data[0]['id']
            logger.info(f"üßπ Limpando dados antigos da simula√ß√£o ID {sim_id}...")
            self.db.client.table("backtest_results").delete().eq("simulation_id", sim_id).execute()
            self.db.client.table("backtest_simulations").update({
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "status": "RUNNING"
            }).eq("id", sim_id).execute()
        else:
            res = self.db.client.table("backtest_simulations").insert({
                "simulation_name": simulation_name,
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "status": "RUNNING"
            }).execute()
            sim_id = res.data[0]['id']

        dates = pd.date_range(start_date, end_date, freq='MS', tz='UTC')
        total_steps = len(dates)
        
        logger.info(f"‚è≥ Executando {total_steps} snapshots mensais...")

        for i, current_date in enumerate(dates):
            if i % max(1, total_steps // 10) == 0:
                logger.info(f"üîÑ Progresso: {current_date.date()} ({i+1}/{total_steps})")
            
            pit_market = full_market.loc[:current_date]
            df_snapshot_climate = self._build_climate_snapshot(contracts, climate_map, current_date)

            snapshot_results = []

            for contract in contracts:
                try:
                    pd_score, metrics = self.engine.calculate_pd_metrics(
                        pit_market, 
                        contract['client_name'],
                        df_snapshot_climate, 
                        contract,
                        current_date.month
                    )

                    # --- CORRE√á√ÉO XAI: GERA√á√ÉO DA NARRATIVA ---
                    # Aqui chamamos o Advisor explicitamente para este snapshot
                    narrative = self.advisor.generate_credit_narrative(pd_score, metrics)
                    # ------------------------------------------

                    lgd = 0.45 
                    expected_loss = (pd_score / 100.0) * lgd * contract['loan_amount']
                    
                    snapshot_results.append({
                        "simulation_id": sim_id,
                        "contract_id": contract['id'],
                        "sim_date": current_date.date().isoformat(),
                        "pd_score": float(pd_score),
                        "ltv_ratio": float(metrics.get('ltv', 0)),
                        "exposure_at_default": float(contract['loan_amount']),
                        "expected_loss": float(expected_loss),
                        "risk_justification": narrative # <--- SALVANDO O TEXTO GERADO
                    })

                except Exception as e:
                    logger.error(f"Erro no contrato {contract.get('client_name')}: {e}")
                    continue

            if snapshot_results:
                self._bulk_save(snapshot_results)

        self._calculate_final_metrics_sql(sim_id)
        logger.info(f"‚úÖ Backtest {simulation_name} finalizado com sucesso.")

    # ... (Mantenha os m√©todos auxiliares _build_climate_snapshot, _calculate_final_metrics_sql, etc. iguais ao anterior)
    def _build_climate_snapshot(self, contracts, climate_map, current_date):
        snapshot_records = []
        lookback_date = (current_date - timedelta(days=7)).strftime('%Y-%m-%d')
        target_date_str = current_date.strftime('%Y-%m-%d')

        for contract in contracts:
            coords = (contract['latitude'], contract['longitude'])
            weather_df = climate_map.get(coords)
            
            if weather_df is not None:
                window = weather_df[
                    (weather_df['date'] <= target_date_str) & 
                    (weather_df['date'] > lookback_date)
                ]
                rain_7d = window['precipitation'].sum() if not window.empty else 0
                temp_max = window['temp_max'].mean() if not window.empty else 25
                
                status, score = self.climate_intel.analyze_risk(
                    {'rain_7d': rain_7d, 'temp_max': temp_max},
                    'production',
                    'S', 
                    current_date.month
                )
                snapshot_records.append({
                    'Location': contract['client_name'],
                    'Risk_Status': status,
                    'Risk_Score': score,
                    'Rain_7d': rain_7d,
                    'Temp_Max': temp_max
                })
        return pd.DataFrame(snapshot_records)

    def _calculate_final_metrics_sql(self, sim_id):
        try:
            all_results = []
            offset = 0
            while True:
                res = self.db.client.table("backtest_results").select("sim_date, expected_loss").eq("simulation_id", sim_id).range(offset, offset + 999).execute()
                if not res.data: break
                all_results.extend(res.data)
                offset += 1000
            
            df_agg = pd.DataFrame(all_results)
            if df_agg.empty: return

            portfolio_monthly_loss = df_agg.groupby('sim_date')['expected_loss'].sum()
            avg_monthly_loss = portfolio_monthly_loss.mean()
            total_cycle_loss = portfolio_monthly_loss.sum()
            portfolio_var_95 = np.percentile(portfolio_monthly_loss, 95)

            self.db.client.table("backtest_simulations").update({
                "total_expected_loss": float(total_cycle_loss), 
                "avg_log_loss": float(avg_monthly_loss),        
                "max_var_95": float(portfolio_var_95), 
                "status": "COMPLETED"
            }).eq("id", sim_id).execute()
        except Exception as e:
            logger.error(f"Erro na agrega√ß√£o: {e}")

    def _load_historical_market(self, start, end):
        start_str = start.strftime('%Y-%m-%d')
        end_str = end.strftime('%Y-%m-%d')
        res = self.db.client.table("market_prices").select("ticker, close, date").gte("date", start_str).lte("date", end_str).execute()
        if not res.data: return None
        df = pd.DataFrame(res.data)
        df['date'] = pd.to_datetime(df['date'], utc=True)
        df_pivot = df.pivot(index='date', columns='ticker', values='close').sort_index().ffill()
        for t in ["ZS=F", "USDBRL=X", "CL=F"]:
            if t not in df_pivot.columns: df_pivot[t] = np.nan
        return df_pivot

    def _load_historical_climate_map(self, contracts, start, end):
        climate_map = {}
        try:
            res = self.db.client.table("climate_historical_cache").select("*").execute()
            active_coords = set((c['latitude'], c['longitude']) for c in contracts)
            for row in res.data:
                coords = (row['latitude'], row['longitude'])
                if coords in active_coords:
                    climate_map[coords] = pd.DataFrame(row['data_json'])
            return climate_map
        except Exception as e:
            logger.error(f"Erro mapa clima: {e}")
            return {}

    def _bulk_save(self, data):
        if not data: return
        try:
            self.db.client.table("backtest_results").insert(data).execute()
        except Exception:
            pass


============================================================
ARQUIVO: ibge_client.py
LOCAL: core/ibge_client.py
============================================================

import httpx
import asyncio
from core.logger import get_logger

logger = get_logger("IBGEClient")

class IBGEClient:
    """
    Consome a API SIDRA do IBGE com Fallback Multi-Safra.
    N√≠vel Ouro: Dados de conting√™ncia espec√≠ficos por ano para evitar vi√©s no backtest.
    """
    BASE_URL = "https://servicodados.ibge.gov.br/api/v3/agregados"

    # MATRIZ DE CONTING√äNCIA HIST√ìRICA (Dados Oficiais Reais)
    FALLBACK_DATA = {
        "2023": { # Safra 23/24 (El Ni√±o/Crise)
            "MT": 3439.0, "PR": 3250.0, "GO": 3520.0, "RS": 3100.0, "MS": 3380.0, "BA": 3650.0
        },
        "2022": { # Safra 22/23 (Ano Neutro/Recorde)
            "MT": 3651.0, "PR": 3423.0, "GO": 3620.0, "RS": 3150.0, "MS": 3580.0, "BA": 4020.0
        }
    }

    async def get_actual_yield(self, state_code: str, year: str):
        state_code = state_code.upper()
        state_map = {"MT": 51, "PR": 41, "GO": 52, "RS": 43, "MS": 50, "BA": 29}
        state_id = state_map.get(state_code)
        
        if not state_id: return None

        # Tentativa via API
        url = f"{self.BASE_URL}/1612/periodos/{year}12/variaveis/35?localidades=N3[{state_id}]&classificacao=81[2702]"
        
        async with httpx.AsyncClient() as client:
            for attempt in range(2): # Reduzido para 2 tentativas para agilizar o fallback
                try:
                    response = await client.get(url, timeout=10.0)
                    if response.status_code == 200:
                        data = response.json()
                        return float(data[0]['resultados'][0]['series'][0]['serie'][f"{year}12"])
                except Exception:
                    continue

        # --- FALLBACK ESTRAT√âGICO POR ANO ---
        logger.error(f"üö® API IBGE Indispon√≠vel. Acionando Conting√™ncia {year} para {state_code}.")
        year_data = self.FALLBACK_DATA.get(year, self.FALLBACK_DATA["2023"])
        return year_data.get(state_code)


============================================================
ARQUIVO: historical_climate_loader.py
LOCAL: core/historical_climate_loader.py
============================================================

import httpx
import asyncio
import hashlib
import pandas as pd
from datetime import datetime
from core.db import DatabaseManager
from core.logger import get_logger

logger = get_logger("HistoricalClimateLoader")

class HistoricalClimateLoader:
    """
    Carregador de Clima Hist√≥rico Real (Open-Meteo Archive).
    Respons√°vel por popular o cache com dados clim√°ticos VERDADEIROS da safra passada.
    """
    
    # API de Arquivo (Dados passados reais, n√£o previs√£o)
    ARCHIVE_URL = "https://archive-api.open-meteo.com/v1/archive"

    def __init__(self, db_manager):
        self.db = db_manager
        # Sem√°foro para n√£o estourar o rate limit da API Open-Meteo
        self.semaphore = asyncio.Semaphore(5) 

    def _generate_hash(self, lat, lon, start, end):
        """Gera ID √∫nico para o cache."""
        # Normaliza datas se vierem como datetime
        if isinstance(start, datetime): start = start.strftime('%Y-%m-%d')
        if isinstance(end, datetime): end = end.strftime('%Y-%m-%d')
        
        content = f"{lat:.2f}_{lon:.2f}_{start}_{end}"
        return hashlib.md5(content.encode()).hexdigest()

    async def fetch_real_history(self, lat, lon, start_date, end_date):
        """
        Busca a verdade clim√°tica hist√≥rica para um ponto espec√≠fico.
        """
        # Garante formato string YYYY-MM-DD
        s_date = start_date.strftime('%Y-%m-%d') if isinstance(start_date, datetime) else start_date
        e_date = end_date.strftime('%Y-%m-%d') if isinstance(end_date, datetime) else end_date

        cache_hash = self._generate_hash(lat, lon, s_date, e_date)
        
        # 1. Check Cache (Evita re-download)
        # Usamos a tabela 'climate_historical_cache' que o BacktestEngine j√° sabe ler
        cached = self.db.client.table("climate_historical_cache")\
            .select("data_json")\
            .eq("coordinate_hash", cache_hash)\
            .execute()
        
        if cached.data:
            # logger.info(f"üì¶ Cache Hit Clima: {lat}, {lon}")
            return pd.DataFrame(cached.data[0]['data_json'])

        # 2. Fetch API Real (Com controle de concorr√™ncia)
        async with self.semaphore:
            params = {
                "latitude": lat,
                "longitude": lon,
                "start_date": s_date,
                "end_date": e_date,
                "daily": ["precipitation_sum", "temperature_2m_max"],
                "timezone": "America/Sao_Paulo"
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                try:
                    resp = await client.get(self.ARCHIVE_URL, params=params)
                    
                    if resp.status_code != 200:
                        logger.warning(f"‚ö†Ô∏è Falha Open-Meteo ({resp.status_code}) para {lat},{lon}")
                        return pd.DataFrame()

                    data = resp.json()
                    
                    # Processamento para formato tabular
                    if 'daily' not in data:
                        return pd.DataFrame()

                    df = pd.DataFrame({
                        'date': data['daily']['time'],
                        'precipitation': data['daily']['precipitation_sum'],
                        'temp_max': data['daily']['temperature_2m_max']
                    })
                    
                    # 3. Persist√™ncia (Cache)
                    self.db.client.table("climate_historical_cache").upsert({
                        "coordinate_hash": cache_hash,
                        "latitude": lat,
                        "longitude": lon,
                        "data_json": df.to_dict(orient='records')
                    }, on_conflict="coordinate_hash").execute()
                    
                    logger.info(f"‚úÖ Clima Real Baixado: {lat:.2f}, {lon:.2f} ({len(df)} dias)")
                    return df

                except Exception as e:
                    logger.error(f"‚ùå Erro API Clima: {e}")
                    return pd.DataFrame()

    async def batch_load(self, contracts, start_date, end_date):
        """
        M√©todo exigido pelo run_backtest.py.
        Carrega dados para todos os contratos de forma eficiente.
        """
        logger.info(f"üåç Iniciando carga em lote de clima real para {len(contracts)} contratos...")
        
        # 1. Deduplica√ß√£o de Coordenadas (Muitos contratos podem estar na mesma fazenda/regi√£o)
        unique_coords = set()
        for c in contracts:
            # Arredonda para 2 casas para agrupar vizinhos pr√≥ximos e economizar API
            lat = round(float(c['latitude']), 2)
            lon = round(float(c['longitude']), 2)
            unique_coords.add((lat, lon))
            
        logger.info(f"üìç Locais √∫nicos identificados: {len(unique_coords)}")

        # 2. Cria√ß√£o das Tarefas Ass√≠ncronas
        tasks = []
        for lat, lon in unique_coords:
            tasks.append(self.fetch_real_history(lat, lon, start_date, end_date))
            
        # 3. Execu√ß√£o Paralela
        await asyncio.gather(*tasks)
        logger.info("‚úÖ Carga de Clima Hist√≥rico conclu√≠da.")


============================================================
ARQUIVO: persister.py
LOCAL: core/persister.py
============================================================

from datetime import datetime
from core.db import DatabaseManager
from core.indicators.financial import calculate_fertilizer_affordability

class RiskPersister:
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        self.now_iso = datetime.now(self.db.tz).isoformat()

    def save_region_risk(self, loc, final_score, raw_scores, metrics):
        self.db.client.table("risk_history").insert({
            "risk_name": loc['name'],
            "status": f"{final_score:.1f}", 
            "risk_level": "CR√çTICO" if final_score > 70 else ("ALERTA" if final_score > 40 else "NORMAL"),
            "region": loc.get('group', 'CREDIT'),
            "category": "LOG√çSTICA" if raw_scores['Log√≠stica'] > 50 else "MERCADO",
            "details": metrics, 
            "created_at": self.now_iso
        }).execute()

    def save_market_metrics(self, df_market, context):
        # FAI Calculation
        fai_status = calculate_fertilizer_affordability(
            df_market['ZS=F'], df_market['NG=F'], df_market['NTR']
        )
        
        self.db.save_market_metrics({
            "basis_risk": context.logistics_benchmark['value'],
            "basis_status": context.logistics_benchmark['status'],
            "china_demand": context.china_metrics,
            "fertilizer_risk": 1.0 if "ALERTA" in fai_status else 0.0, 
            "fai_status": fai_status 
        })

    def save_global_state(self, context, macro_corr):
        main_driver = max(context.pillar_sums, key=context.pillar_sums.get) if context.processed_count > 0 else "Clima"
        
        # Tend√™ncia 7d
        try:
            res_hist = self.db.client.table("risk_history").select("status").eq("risk_name", "GLOBAL_SCORE").order("created_at", desc=True).limit(7).execute()
            avg_7d = sum([float(r['status']) for r in res_hist.data]) / len(res_hist.data) if res_hist.data else 0
            trend_vs_7d = ((context.avg_global_score / avg_7d) - 1) if avg_7d > 0 else 0
        except Exception:
            trend_vs_7d = 0.0

        # Recomenda√ß√£o
        if context.critical_clusters:
            cluster_str = " | ".join(context.critical_clusters)
            rec = f"CR√çTICO: Anomalia Sist√™mica em {cluster_str}. A√ß√£o imediata."
        elif context.avg_global_score > 70:
            rec = f"ALERTA GLOBAL: Risco sist√™mico elevado em {main_driver}."
        else:
            rec = f"EST√ÅVEL: Opera√ß√£o nominal. Monitorando {len(context.cluster_scores)} clusters."

        self.db.client.table("risk_history").insert({
            "risk_name": "GLOBAL_SCORE",
            "status": f"{context.avg_global_score:.1f}",
            "risk_level": "CR√çTICO" if context.critical_clusters or context.avg_global_score > 70 else ("ALERTA" if context.avg_global_score > 45 else "NORMAL"),
            "region": "GLOBAL",
            "category": "MARKET",
            "details": {
                "washout_risk": context.max_washout_risk,
                "logistics_benchmark": context.logistics_benchmark,
                "china_metrics": context.china_metrics,
                "main_driver": main_driver,
                "trend_7d": f"{trend_vs_7d:+.1%}",
                "macro_stress": f"{macro_corr:.2f}",
                "analyst_recommendation": rec,
                "critical_clusters": context.critical_clusters
            },
            "created_at": self.now_iso
        }).execute()

    def save_contract_risk(self, contract, pd_score, metrics):
        """
        Salva o hist√≥rico de risco com rastreabilidade total (Audit Trail).
        """
        try:
            payload = {
                "contract_id": contract['id'],
                "risk_name": contract['name'],
                "status": str(pd_score),
                "risk_level": self._get_risk_level(pd_score),
                "category": "CREDIT_PD",
                "details": metrics,
                "created_at": self.now_iso
            }
            
            res = self.db.client.table("risk_history").insert(payload).execute()
            
            # Verifica√ß√£o institucional de sucesso
            if hasattr(res, 'status_code') and res.status_code >= 400:
                logger.error(f"DB Error {res.status_code}: {res.data}")
                
        except Exception as e:
            # Log detalhado para debug r√°pido
            logger.error(f"‚ö†Ô∏è Falha na persist√™ncia: {str(e)}")
            # Em n√≠vel institucional, poder√≠amos implementar um fallback para arquivo local aqui

    def _get_risk_level(self, score):
        if score > 75: return "CRITICAL"
        if score > 50: return "HIGH"
        if score > 25: return "MODERATE"
        return "LOW"


============================================================
ARQUIVO: market_router.py
LOCAL: core/market_router.py
============================================================

# core/market_router.py
from core.brapi_client import BrapiClient
import yfinance as yf
import pandas as pd
from core.logger import get_logger

logger = get_logger("MarketRouter")

class MarketRouter:
    def __init__(self, config):
        self.config = config
        self.brapi = BrapiClient()
        self.sources_map = config.get('market_sources', {}).get('overrides', {})
        self.translations = config.get('ticker_translation', {})

    def fetch_batch(self, tickers):
        """
        Orquestra a busca dividindo os tickers entre os provedores corretos.
        Retorna um DataFrame unificado e normalizado.
        """
        brapi_tickers = []
        yahoo_tickers = []
        
        # 1. Roteamento
        for t in tickers:
            source = self.sources_map.get(t, 'yahoo') # Default √© Yahoo
            if source == 'brapi':
                brapi_tickers.append(t)
            else:
                yahoo_tickers.append(t)

        results = []

        # 2. Execu√ß√£o Brapi (Iterativo pois a API free pode n√£o suportar batch complexo)
        for t in brapi_tickers:
            # Traduz o ticker do sistema para o ticker da API
            api_symbol = self.translations.get('brapi', {}).get(t, t)
            logger.info(f"üáßüá∑ Roteando {t} -> Brapi ({api_symbol})")
            
            df = self.brapi.get_historical_data(api_symbol) # M√©todo que criamos antes
            if not df.empty:
                df['ticker'] = t # Garante que o DF tenha o ticker do sistema
                df['source'] = 'BRAPI' # AUDITABILIDADE (Essencial para Institucional)
                results.append(df)
            else:
                logger.warning(f"‚ö†Ô∏è Falha Brapi para {t}. Tentando Fallback Yahoo.")
                yahoo_tickers.append(t) # Fallback autom√°tico

        # 3. Execu√ß√£o Yahoo (Batch)
        if yahoo_tickers:
            logger.info(f"üá∫üá∏ Roteando {len(yahoo_tickers)} ativos -> Yahoo Finance")
            try:
                # Otimiza√ß√£o: Threads=True
                yf_data = yf.download(yahoo_tickers, period="5d", progress=False, threads=True, group_by='ticker')
                
                # Normaliza√ß√£o chata do Yahoo MultiIndex
                for t in yahoo_tickers:
                    try:
                        df_t = yf_data[t].dropna().copy() if len(yahoo_tickers) > 1 else yf_data.dropna().copy()
                        if not df_t.empty:
                            df_t['ticker'] = t
                            df_t['source'] = 'YAHOO'
                            results.append(df_t)
                    except KeyError:
                        logger.error(f"Yahoo n√£o retornou dados para {t}")
            except Exception as e:
                logger.error(f"Erro cr√≠tico Yahoo: {e}")

        # 4. Unifica√ß√£o
        if not results:
            return pd.DataFrame()
            
        return pd.concat(results)


============================================================
ARQUIVO: mt_strategy.py
LOCAL: core/strategies/mt_strategy.py
============================================================

from .base import BaseRiskStrategy
from core.indicators.technical import TechnicalIndicators as Tech
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)

class MatoGrossoStrategy(BaseRiskStrategy):
    def __init__(self):
        super().__init__()
        self.region_name = "Mato Grosso"
        self.state_code = "MT" 

    def calculate_logistics_risk(self, df_market: pd.DataFrame, loc_data: dict) -> float:
        """
        Calculates logistics risk focusing on DISTANCE to penalize MT relative to PR.
        
        Logic:
        1. Base Risk = Linear distance (1 point per 50km).
        2. Multiplier = If Diesel (Oil) prices are rising, long haul costs skyrocket.
        """
        # 1. Get Distance (Default to 2000km for MT if missing)
        dist_port = loc_data.get('dist_to_port', 2000)
        
        # 2. Base Calculation (The "Descolamento" Factor)
        # MT (2000km) / 50 = 40.0 Risk Score
        # PR (500km)  / 50 = 10.0 Risk Score
        logistics_score = dist_port / 50.0

        # 3. Diesel Aggravator
        # We check Crude Oil (CL=F) as a proxy for Diesel costs
        diesel_series = df_market['CL=F']
        diesel_change = diesel_series.pct_change().iloc[-1] if len(diesel_series) > 1 else 0
        
        # If Diesel prices are rising AND the distance is long (>1000km)
        # We amplify the risk for MT, but PR (being <1000km) stays closer to base.
        if diesel_change > 0 and dist_port > 1000:
            logistics_score *= 1.5  # 50% penalty for rising fuel costs on long routes
            
        return self.sanitize_score(logistics_score)

    def calculate_climate_risk(self, df_climate: pd.DataFrame, contract_data: dict, month: int) -> float:
        # Uses 'name' mapped in pipeline
        loc_id = contract_data.get('name') 
        
        if df_climate is None or df_climate.empty:
             return 10.0 # Safe Fallback
             
        loc_row = df_climate[df_climate['Location'] == loc_id]
        
        if loc_row.empty:
            return 10.0 
        
        return float(loc_row.iloc[0]['Risk_Score'])

    def calculate_market_risk(self, df_market: pd.DataFrame) -> float:
        """
        Market Risk based on Backtest findings: RSI and Volatility.
        """
        # 1. Recover Indicators
        rsi = Tech.calculate_rsi(df_market['ZS=F'])
        vol = Tech.calculate_volatility(df_market['ZS=F'])
        
        # 2. Logic: Stretched market (High RSI) or High Volatility increases risk
        market_score = 30.0 # Base Market Risk
        
        if not pd.isna(rsi) and rsi > 70: 
            market_score += 40  # Overbought territory (High risk of correction)
            
        if not pd.isna(vol) and vol > 0.30: 
            market_score += 30 # High volatility
        
        return self.sanitize_score(market_score)


============================================================
ARQUIVO: pr_strategy.py
LOCAL: core/strategies/pr_strategy.py
============================================================

from .base import BaseRiskStrategy
from core.indicators.technical import TechnicalIndicators as Tech
import pandas as pd
import logging

logger = logging.getLogger(__name__)

class ParanaStrategy(BaseRiskStrategy):
    def __init__(self):
        super().__init__()
        self.region_name = "Paran√°"
        self.state_code = "PR" # Vincula√ß√£o expl√≠cita

    def calculate_logistics_risk(self, df_market: pd.DataFrame, loc_data: dict) -> float:
        # PR tem custo log√≠stico menor e mais est√°vel
        return self.sanitize_score(15.0)

    def calculate_climate_risk(self, df_climate: pd.DataFrame, contract_data: dict, month: int) -> float:
        # Agora contract_data['name'] sempre existir√° por causa do Mapper no Pipeline
        loc_id = contract_data.get('name') 
        loc_row = df_climate[df_climate['Location'] == loc_id]
        
        if loc_row.empty:
            return 10.0 # Fallback seguro
        
        return float(loc_row.iloc[0]['Risk_Score'])

    def calculate_market_risk(self, df_market: pd.DataFrame) -> float:
        # 1. Recuperamos os indicadores que o backtest provou que funcionam
        soy_brl = self.get_soy_brl_price(df_market) # Pre√ßo em R$
        rsi = Tech.calculate_rsi(df_market['ZS=F'])
        vol = Tech.calculate_volatility(df_market['ZS=F'])
        
        # 2. L√≥gica do Backtest: Se o mercado est√° esticado (RSI alto) 
        # ou vol√°til demais, o risco de mercado sobe.
        market_score = 30.0 # Base - Mantendo alinhado com o MT base
        
        if rsi > 70: market_score += 40  # Sobrecompra (Risco de queda)
        if vol > 0.30: market_score += 30 # Alta volatilidade
        
        return self.sanitize_score(market_score)


============================================================
ARQUIVO: base.py
LOCAL: core/strategies/base.py
============================================================

from abc import ABC, abstractmethod
import pandas as pd
import numpy as np
import yaml

class BaseRiskStrategy(ABC):
    def __init__(self):
        self.region_name = "Base"
        self.settings = self._load_settings()

    def _load_settings(self) -> dict:
        """
        Carrega as configura√ß√µes do arquivo settings.yaml.
        """
        try:
            with open("/home/obscuritenoir/Portfolio/credit-risk-engine/settings.yaml", "r") as f:
                return yaml.safe_load(f)
        except Exception as e:
            raise RuntimeError(f"Erro ao carregar settings.yaml: {e}")

    def get_data_source(self, ticker: str) -> str:
        """
        Retorna a fonte de dados para um ticker espec√≠fico com base nas configura√ß√µes.
        """
        overrides = self.settings.get("market_sources", {}).get("overrides", {})
        return overrides.get(ticker, self.settings.get("market_sources", {}).get("default", "yahoo"))

    def translate_ticker(self, ticker: str, source: str) -> str:
        """
        Traduz o ticker para o formato esperado pela fonte de dados.
        """
        translation_map = self.settings.get("ticker_translation", {}).get(source, {})
        return translation_map.get(ticker, ticker)

    @abstractmethod
    def calculate_logistics_risk(self, df_market: pd.DataFrame, contract_data: dict) -> float:
        """
        Calcula o risco log√≠stico com base nos dados de mercado e contrato.

        Par√¢metros:
        - df_market: DataFrame contendo os dados de mercado.
        - contract_data: Dicion√°rio que deve conter a chave 'dist_to_port', representando a dist√¢ncia ao porto.

        Retorna:
        - Risco log√≠stico como um valor float.
        """
        pass

    @abstractmethod
    def calculate_climate_risk(self, df_climate: pd.DataFrame, contract_data: dict, month: int) -> float:
        """
        Calcula o risco clim√°tico com base nos dados clim√°ticos, contrato e m√™s.

        Par√¢metros:
        - df_climate: DataFrame contendo os dados clim√°ticos.
        - contract_data: Dicion√°rio que deve conter a chave 'identifier', que mapeia para 'Location' no df_climate.
        - month: Inteiro representando o m√™s para o qual o risco clim√°tico ser√° calculado.

        Retorna:
        - Risco clim√°tico como um valor float.
        """
        pass

    @abstractmethod
    def calculate_market_risk(self, df_market: pd.DataFrame) -> float:
        pass

    def get_soy_brl_price(self, df_market: pd.DataFrame) -> float:
        """
        Retorna o pre√ßo da saca de 60kg em BRL.
        Base institucional para c√°lculo de valor de garantia.
        """
        try:
            soy_chicago = df_market['ZS=F'].iloc[-1] 
            usd_brl = df_market['USDBRL=X'].iloc[-1]
            
            # Convers√£o: (Cents/Bushel / 100) * USD * 2.2046 (Bushels to 60kg Bag)
            price_saca = (soy_chicago / 100) * usd_brl * 2.2046
            return round(float(price_saca), 2)
        except:
            return 0.0

    def sanitize_score(self, score: float) -> float:
        """Garante que o score n√£o saia do range 0-100"""
        return float(np.clip(score, 0.0, 100.0))


============================================================
ARQUIVO: presenter.py
LOCAL: core/reporting/presenter.py
============================================================

class HTMLPresenter:
    """
    Respons√°vel pela formata√ß√£o visual do relat√≥rio (HTML).
    Isola a camada de apresenta√ß√£o da l√≥gica de neg√≥cios.
    """

    @staticmethod
    def build_narrative_html(df_climate):
        """
        Converte os dados clim√°ticos brutos em blocos HTML formatados.
        """
        narrative_groups = {"production": [], "logistics": [], "global": []}
        
        if df_climate is None or df_climate.empty: 
            return narrative_groups

        # Ordena√ß√£o Visual: Vermelho (1) -> Amarelo (2) -> Verde (3)
        df_sorted = df_climate.copy()
        df_sorted['sort_key'] = df_sorted['Risk_Status'].apply(
            lambda x: 1 if 'üî¥' in str(x) else (2 if 'üü°' in str(x) else 3)
        )
        df_sorted = df_sorted.sort_values('sort_key')
        
        seen_tags = set()
        
        # Keywords de Categoriza√ß√£o Visual
        kw_logistics = [
            'SANTOS', 'PARANAGUA', 'ITAQUI', 'RIO GRANDE', 'PORTO', 
            'BR-163', 'FERROVIA', 'BR-364', 'HIDROVIA', 'PANAMA', 'SUEZ', 
            'RHINE', 'ROTTERDAM'
        ]
        kw_production = [
            'MT', 'PR', 'GO', 'MS', 'SORRISO', 'SINOP', 'LUCAS', 
            'CASCAVEL', 'RIO VERDE', 'MATO', 'PARANA', 'BAHIA',
            'IOWA', 'ILLINOIS', 'DES MOINES', 'CORN BELT'
        ]

        for _, row in df_sorted.iterrows():
            status = str(row['Risk_Status'])
            
            # Filtro de Relev√¢ncia Visual (Ignora Verde/Branco no texto)
            if 'üü¢' in status or '‚ö™' in status: continue

            loc = str(row['Location']).upper()
            if loc in seen_tags: continue
            seen_tags.add(loc)

            # Estiliza√ß√£o
            icon = "üö®" if "üî¥" in status else "‚ö†Ô∏è"
            color = "#cf222e" if "üî¥" in status else "#d29922"
            
            item_html = f"<li style='margin-bottom: 5px; color: #24292f;'><strong>{icon} {loc}:</strong> <span style='color: {color};'>{status}</span></li>"
            
            if any(k in loc for k in kw_logistics): 
                narrative_groups['logistics'].append(item_html)
            elif any(k in loc for k in kw_production): 
                narrative_groups['production'].append(item_html)
            else: 
                narrative_groups['global'].append(item_html)

        return narrative_groups


============================================================
ARQUIVO: macro.py
LOCAL: core/indicators/macro.py
============================================================

import pandas as pd
import numpy as np

class MacroIndicators:
    @staticmethod
    def calculate_currency_stress(usd_series: pd.Series) -> dict:
        """[RECUPERADO v2.8.2] Mede desvio de volatilidade cambial vs M√©dia Hist√≥rica."""
        try:
            returns = usd_series.pct_change().dropna()
            current_vol = returns.tail(21).std() * np.sqrt(252)
            hist_vol = returns.rolling(252).std().mean() * np.sqrt(252)
            ratio = current_vol / hist_vol
            
            score = 80 if ratio > 1.5 else (40 if ratio > 1.2 else 0)
            status = "CR√çTICO" if ratio > 1.5 else ("ALERTA" if ratio > 1.2 else "EST√ÅVEL")
            return {"score": score, "status": status, "ratio": round(ratio, 2)}
        except:
            return {"score": 0, "status": "NEUTRO"}

    @staticmethod
    def calculate_geopolitical_risk(gold_series: pd.Series, oil_series: pd.Series) -> dict:
        """[RECUPERADO v2.8.2] Detector de Cisne Negro (Diverg√™ncia Ouro/Oil)."""
        try:
            corr = gold_series.tail(20).corr(oil_series.tail(20))
            g_ret = (gold_series.iloc[-1] / gold_series.iloc[-5]) - 1
            o_ret = (oil_series.iloc[-1] / oil_series.iloc[-5]) - 1

            if g_ret > 0.03 and o_ret > 0.05 and corr < 0.3:
                return {"score": 90, "status": "CR√çTICO (Cisne Negro Detectado)"}
            return {"score": 0, "status": "EST√ÅVEL"}
        except:
            return {"score": 0, "status": "NEUTRO"}


============================================================
ARQUIVO: __init__.py
LOCAL: core/indicators/__init__.py
============================================================

from .technical import TechnicalIndicators
from .financial import FinancialIndicators
from .macro import MacroIndicators

class AgroIndicators(TechnicalIndicators, FinancialIndicators, MacroIndicators):
    """
    Fachada Unificada (Facade Pattern).
    Agrega todas as estrat√©gias (T√©cnica, Financeira, Macro) em uma √∫nica classe
    para facilitar o uso pelo Engine.
    """
    pass


============================================================
ARQUIVO: financial.py
LOCAL: core/indicators/financial.py
============================================================

import pandas as pd

class FinancialIndicators:
    """
    Especialista em Margens, Paridade, Rela√ß√£o de Troca e Estrutura de Mercado.
    Foca na viabilidade econ√¥mica da cadeia 'Farm-to-Port'.
    """

    @staticmethod
    def theoretical_parity(cbot_usd_bu: float, usd_brl: float) -> float:
        """
        Pre√ßo te√≥rico da saca em BRL (Chicago * D√≥lar * Fator Convers√£o).
        Nota: Futuramente adicionaremos o Pr√™mio de Porto aqui.
        """
        return cbot_usd_bu * usd_brl * 2.2046

    @staticmethod
    def calculate_soy_crush_margin(soy_price, meal_price, oil_price):
        """
        [FASE 2] Calcula a Margem de Esmagamento Te√≥rica (Crush Margin).
        Standard Yield: 1 bushel de soja (~60lb) produz ~44lb de farelo e ~11lb de √≥leo.
        """
        try:
            # 1. Contribui√ß√£o do Farelo: pre√ßo em USD/ton curta (2000 lbs) -> converter para 44lb
            meal_contribution = (meal_price / 2000) * 44
            
            # 2. Contribui√ß√£o do √ìleo: pre√ßo em centavos/lb -> converter para 11lb e transformar em USD
            oil_contribution = (oil_price / 100) * 11
            
            # 3. Valor Bruto dos Subprodutos (Gross Processing Value)
            gross_value = meal_contribution + oil_contribution
            
            # 4. Margem Final: Valor Bruto - Custo da Soja (USD/bushel)
            crush_margin = gross_value - soy_price
            
            # Status de Atratividade para a Ind√∫stria Processadora
            if crush_margin > 2.5: 
                status = "EXCELENTE (Ind√∫stria Compradora)"
            elif crush_margin > 1.0: 
                status = "POSITIVA"
            else: 
                status = "CR√çTICA (Risco de Parada de F√°brica)"
            
            return {
                "value": round(crush_margin, 2),
                "status": status
            }
        except Exception:
            return {"value": 0, "status": "ERRO NO C√ÅLCULO"}

    @staticmethod
    def calculate_market_structure(current_contract_price, future_contract_price):
        """
        [FASE 2 - MAESTRIA] Detecta se o mercado est√° em Carry (Normal) ou Inverse (Risco).
        A estrutura de tela indica se vale a pena armazenar o gr√£o ou vender imediatamente.
        """
        try:
            spread = future_contract_price - current_contract_price
            
            # Se o pre√ßo futuro √© menor que o atual, o mercado est√° "Invertido"
            # Isso sinaliza escassez aguda e risco de o produtor ser for√ßado a vender na colheita.
            if spread < 0:
                return {
                    "status": "INVERSE (Risco de Armazenagem)",
                    "spread": round(spread, 2),
                    "risk_weight": 30 # Penalidade sugerida para o score
                }
            
            return {
                "status": "CARRY (Estrutura Normal)",
                "spread": round(spread, 2),
                "risk_weight": 0
            }
        except Exception:
            return {"status": "NEUTRO", "spread": 0, "risk_weight": 0}

    @staticmethod
    def calculate_terms_of_trade(revenue_series: pd.Series, cost_series: pd.Series) -> dict:
        """
        Efeito Tesoura (Terms of Trade).
        Receita (Soja) vs Custo (Insumo/Petr√≥leo).
        """
        # Alinha as datas para garantir c√°lculo correto
        df = pd.concat([revenue_series, cost_series], axis=1).dropna()
        if df.empty: return {"ratio_rsi": 50, "trend": "NEUTRO"}
        
        rev = df.iloc[:, 0]
        cost = df.iloc[:, 1]
        ratio = cost / rev # Quanto maior, pior (custo pesa mais que receita)
        
        # RSI do Ratio (Velocidade da mudan√ßa de custo)
        delta = ratio.diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss.replace(0, 0.001)
        ratio_rsi = 100 - (100 / (1 + rs)).iloc[-1]
        
        return {
            "current_ratio": ratio.iloc[-1],
            "ratio_rsi": ratio_rsi,
            "trend": "PIORANDO" if ratio.iloc[-1] > ratio.mean() else "MELHORANDO"
        }
    
    # No core/indicators/financial.py
@staticmethod
def calculate_fertilizer_affordability(soy_series, gas_series, stock_series):
    """
    [REFINAMENTO QUANT] Implementa Lookback de 180 dias (6 meses) para Insumos.
    Simula o custo de aquisi√ß√£o real vs pre√ßo de An√°lise atual.
    """
    # Pre√ßo de An√°lise atual (Soja hoje)
    soy_current = soy_series.iloc[-1]
    
    # Pre√ßo de custo hist√≥rico (Insumos 6 meses atr√°s)
    # Assumindo que o DataFrame tem dados di√°rios, 180 dias = ~6 meses
    if len(gas_series) > 180:
        gas_cost = gas_series.shift(180).iloc[-1]
        stock_cost = stock_series.shift(180).iloc[-1]
    else:
        # Fallback para o in√≠cio da s√©rie se n√£o houver 180 dias
        gas_cost = gas_series.iloc[0]
        stock_cost = stock_series.iloc[0]

    # C√°lculo do √çndice de Acessibilidade (Affordability)
    # Se a soja subiu mais que o custo travado do adubo = Margem Positiva
    affordability_ratio = soy_current / ((gas_cost + stock_cost) / 2)
    
    if affordability_ratio < 0.8: return "ALERTA: Margem de Insumos Cr√≠tica (Custo Travado Alto)"
    if affordability_ratio > 1.2: return "OPORTUNIDADE: Barter Favor√°vel (Custo Travado Baixo)"
    return "EST√ÅVEL"


============================================================
ARQUIVO: fundamental.py
LOCAL: core/indicators/fundamental.py
============================================================

# core/indicators/fundamental.py
import pandas as pd
import numpy as np

class FundamentalIndicators:
    @staticmethod
    def calculate_washout_probability(climate_risk_level, price_trend, price_change_30d):
        """[RECUPERADO v2.8.2] Calcula risco de default/quebra de contrato."""
        try:
            risk_score = 0
            # Fator 1: Risco Biol√≥gico (Quebra de Safra)
            if climate_risk_level == "CR√çTICO": risk_score += 60
            elif climate_risk_level == "ALERTA": risk_score += 20
                
            # Fator 2: Incentivo Econ√¥mico (Default Estrat√©gico)
            if price_change_30d > 0.10 and price_trend == "ALTA": risk_score += 40
            elif price_trend == "ALTA": risk_score += 15
            elif price_trend == "BAIXA": risk_score -= 10
                
            status = "BAIXO"
            if risk_score >= 80: status = "M√ÅXIMO (Risco de Default Iminente)"
            elif risk_score >= 50: status = "ALTO (Monitorar Contrapartes)"
            elif risk_score >= 20: status = "MODERADO"
            
            return {"score": max(0, min(100, risk_score)), "status": status}
        except:
            return {"score": 0, "status": "DADOS INSUFICIENTES"}

    @staticmethod
    def calculate_china_demand(soy_series, hog_series):
        """[FIX: DYNAMIC BASE] Suaviza√ß√£o para evitar vi√©s de ponto √∫nico."""
        try:
            base_soy = soy_series.head(20).mean()
            base_hog = hog_series.head(20).mean()
            soy_norm = soy_series / base_soy * 100
            hog_norm = hog_series / base_hog * 100
            margin_proxy = hog_norm.iloc[-1] - soy_norm.iloc[-1]
            
            if margin_proxy < -15: return {"score": 90, "status": "BAIXA (Risco Washout)"}
            return {"score": 20, "status": "NORMAL"}
        except:
            return {"score": 0, "status": "DADOS INSUFICIENTES"}

    @staticmethod
    def calculate_basis_proxy(volatility, usd_ret, china_demand, is_stale, price_trend="LATERAL"):
        """[RECUPERADO v2.8.2] Basis Sint√©tico com Filtro Direcional."""
        fx_pressure = abs(usd_ret) if usd_ret < -0.5 else 0
        stale_penalty = 2.0 if is_stale else 1.0
        vol_multiplier = 0.5 if price_trend == "BAIXA" else 1.5
        china_factor = 20 if "ALTA" in str(china_demand) else (-10 if "BAIXA" in str(china_demand) else 0)
        risk_score = ((volatility * 100 * vol_multiplier) + (fx_pressure * 3.0) + china_factor) * stale_penalty
        return min(100, risk_score)


============================================================
ARQUIVO: technical.py
LOCAL: core/indicators/technical.py
============================================================

import pandas as pd
import numpy as np

class TechnicalIndicators:
    """
    Especialista em An√°lise T√©cnica (Gr√°ficos e Estat√≠stica).
    """

    @staticmethod
    def calculate_rsi(series: pd.Series, window: int = 14) -> float:
        """Calcula RSI (√çndice de For√ßa Relativa)."""
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss.replace(0, 0.001)
        return 100 - (100 / (1 + rs)).iloc[-1]

    @staticmethod
    def calculate_volatility(series: pd.Series, window: int = 21) -> float:
        """Volatilidade anualizada (Janela de 21 dias √∫teis)."""
        return series.pct_change(fill_method=None).rolling(window).std().iloc[-1] * np.sqrt(252)

    @staticmethod
    def analyze_trend(series: pd.Series, short_window=9, long_window=21) -> str:
        """Tend√™ncia via Cruzamento de M√©dias (EMA)."""
        ema_short = series.ewm(span=short_window, adjust=False).mean().iloc[-1]
        ema_long = series.ewm(span=long_window, adjust=False).mean().iloc[-1]
        if ema_short > ema_long: return "ALTA"
        elif ema_short < ema_long: return "BAIXA"
        return "LATERAL"


============================================================
ARQUIVO: run_backtest.py
LOCAL: scripts/run_backtest.py
============================================================

import asyncio
import argparse
import pandas as pd
from datetime import datetime
from core.db import DatabaseManager
from core.engine import RiskEngine
from core.backtest_engine import InstitutionalBacktestEngine
from core.historical_climate_loader import HistoricalClimateLoader
from core.logger import get_logger

# Configura√ß√£o de Telemetria Institucional
logger = get_logger("BacktestMaestro")

async def execute_institutional_backtest(tag: str):
    """
    Executa a simula√ß√£o Walk-Forward baseada em uma Simulation Tag.
    Garante rastreabilidade total e isolamento de dados.
    """
    logger.info(f"üèõÔ∏è Iniciando Maestro de Backtest | Tag: {tag}")
    
    # 1. INICIALIZA√á√ÉO DE COMPONENTES
    db = DatabaseManager(use_service_role=True)
    engine = RiskEngine()
    climate_loader = HistoricalClimateLoader(db)
    backtester = InstitutionalBacktestEngine(engine, db)

    # 2. FILTRAGEM DE CARTEIRA (Compliance: Isolamento por Tag)
    try:
        res = db.client.table("credit_portfolio")\
            .select("*")\
            .eq("simulation_tag", tag)\
            .execute()
        
        contracts = res.data
        if not contracts:
            logger.error(f"‚ùå Nenhum contrato encontrado para a tag: {tag}")
            return
        
        # CORRE√á√ÉO GOLD: Calcular Exposi√ß√£o Real aqui
        real_exposure = sum(float(c['loan_amount']) for c in contracts)
        logger.info(f"üìä Carteira carregada: {len(contracts)} contratos. EAD Total: R$ {real_exposure:,.2f}")

    except Exception as e:
        logger.error(f"‚ùå Erro ao acessar base de contratos: {e}")
        return

    # 3. CONFIGURA√á√ÉO DO CEN√ÅRIO TEMPORAL (N√≠vel Ouro: For√ßando UTC)
    # Usamos utc=True para garantir compatibilidade com os dados do banco
    start_date = pd.to_datetime("2023-09-01", utc=True)
    end_date = pd.to_datetime("2024-04-30", utc=True)

    # O pd.date_range dentro do backtester herdar√° o fuso hor√°rio de start_date

    # 4. INGEST√ÉO DE CLIMA HIST√ìRICO (Archive API com Cache)
    logger.info(f"üì° Sincronizando Clima Hist√≥rico (Point-in-Time)...")
    await climate_loader.batch_load(contracts, "2023-09-01", "2024-04-30")

    # 5. EXECU√á√ÉO DO MOTOR DE SIMULA√á√ÉO
    # O simulation_name no backtest_results ser√° a pr√≥pria tag para auditoria
    try:
        backtester.run_walk_forward(
            simulation_name=tag,
            start_date=start_date,
            end_date=end_date,
            contracts=contracts
        )
    except Exception as e:
        logger.critical(f"üí• Falha na execu√ß√£o da simula√ß√£o: {e}", exc_info=True)
        return

    # 6. SUM√ÅRIO EXECUTIVO DE RISCO (VaR e Expected Loss)
    # Passamos a exposi√ß√£o real para o relat√≥rio
    _print_institutional_report(db, tag, real_exposure)

def _print_institutional_report(db, tag, real_exposure):
    """
    Gera o report final de performance do modelo para o comit√™ de risco.
    """
    res = db.client.table("backtest_simulations")\
        .select("*")\
        .eq("simulation_name", tag)\
        .execute()
    
    if res.data:
        sim = res.data[0]
        
        # Leitura expl√≠cita dos campos corretos
        avg_el_monthly = sim.get('avg_log_loss', 0)        # M√©dia mensal
        var_95 = sim.get('max_var_95', 0)

        # Severidade Realista: Baseada na M√©dia Mensal vs Exposi√ß√£o
        severity = (avg_el_monthly / real_exposure) if real_exposure > 0 else 0

        print("\n" + "‚ñà"*60)
        print(f"  RELAT√ìRIO DE PERFORMANCE DE MODELO - {tag}")
        print("  " + "‚îÄ"*56)
        print(f"  PER√çODO SIMULADO: {sim['start_date'][:10]} a {sim['end_date'][:10]}")
        print(f"  EXPOSI√á√ÉO (EAD):  R$ {real_exposure:,.2f}")
        print("  " + "‚îÄ"*56)
        print(f"  EXPECTED LOSS (M√âDIA MENSAL): R$ {avg_el_monthly:,.2f}")
        print(f"  VaR (95% MENSAL):             R$ {var_95:,.2f}")
        print("  " + "‚îÄ"*56)
        print(f"  SEVERIDADE AJUSTADA (LGD 45%): {severity:.2%}")
        print(f"  STATUS:           MODELO VALIDADO (AUDIT TRAIL OK)")
        print("‚ñà" * 60 + "\n")

if __name__ == "__main__":
    # Uso via CLI para integra√ß√£o com pipelines de CI/CD
    parser = argparse.ArgumentParser(description="Agro Risk Institutional Backtest")
    parser.add_argument("--tag", required=True, help="Simulation Tag gerada pelo generate_stress_portfolio.py")
    args = parser.parse_args()

    asyncio.run(execute_institutional_backtest(args.tag))


============================================================
ARQUIVO: ingest_historical_market.py
LOCAL: scripts/ingest_historical_market.py
============================================================

import pandas as pd
import yfinance as yf
from datetime import datetime
from core.db import DatabaseManager
from core.env import load_config
from core.logger import get_logger

logger = get_logger("HistoricalMarketIngestor")

def ingest_historical_prices():
    """
    Popula a tabela market_prices com dados reais da Safra 23/24.
    N√≠vel Ouro: Garante que o backtest rode sobre pre√ßos hist√≥ricos reais.
    """
    db = DatabaseManager(use_service_role=True)
    config = load_config()
    
    # Tickers essenciais para o motor de risco
    tickers = config.get('tickers', ["ZS=F", "USDBRL=X", "CL=F"])
    
    # Janela da Safra 23/24 (com margem de seguran√ßa para m√©dias m√≥veis)
    start_date = "2023-01-01"
    end_date = "2024-06-01"

    logger.info(f"üìÖ Buscando dados hist√≥ricos para: {tickers}")

    try:
        # Download massivo via Yahoo Finance
        data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')
        
        records_to_upsert = []

        for ticker in tickers:
            df_ticker = data[ticker].dropna()
            logger.info(f"üìà Processando {ticker}: {len(df_ticker)} registros encontrados.")

            for date, row in df_ticker.iterrows():
                records_to_upsert.append({
                    "ticker": ticker,
                    "date": date.strftime('%Y-%m-%d'),
                    "open": float(row['Open']),
                    "high": float(row['High']),
                    "low": float(row['Low']),
                    "close": float(row['Close']),
                    "volume": float(row['Volume']) if 'Volume' in row else 0
                })

        if records_to_upsert:
            # Upsert institucional: evita duplicatas e garante integridade
            logger.info(f"üöÄ Enviando {len(records_to_upsert)} registros para o Supabase...")
            db.client.table("market_prices").upsert(
                records_to_upsert, 
                on_conflict="ticker, date"
            ).execute()
            logger.info("‚úÖ Dados hist√≥ricos de mercado integrados com sucesso.")
        
    except Exception as e:
        logger.error(f"‚ùå Falha na ingest√£o hist√≥rica: {e}")

if __name__ == "__main__":
    ingest_historical_prices()


============================================================
ARQUIVO: ingest_conab.py
LOCAL: scripts/ingest_conab.py
============================================================

import asyncio
import pandas as pd
from datetime import datetime
from pathlib import Path
from core.db import DatabaseManager
from core.logger import get_logger

logger = get_logger("ConabExactIngestor")

class ConabExactIngestor:
    """
    ETL Cir√∫rgico: Baseado na imagem fornecida do arquivo SojaSerieHist.xls.
    Corre√ß√£o v2: Ajuste de magnitude decimal e Sanity Check.
    """

    TARGET_STATES = ["MT", "PR", "GO", "RS", "MS", "BA", "TO"]
    TARGET_COL_NAME = "2023/24" 

    def __init__(self, file_path: str):
        self.db = DatabaseManager(use_service_role=True)
        self.file_path = Path(file_path)

    def _clean_number(self, value):
        """
        Limpeza robusta para formatos brasileiros mistos no Excel.
        """
        if pd.isna(value) or value == '-':
            return 0.0
        
        try:
            # Converte para string para limpar caracteres
            val_str = str(value).strip()
            
            # Remove pontos de milhar (3.697,5 -> 3697,5)
            val_str = val_str.replace('.', '')
            
            # Troca v√≠rgula decimal por ponto (3697,5 -> 3697.5)
            val_str = val_str.replace(',', '.')
            
            float_val = float(val_str)
            
            # --- SANITY CHECK (A Corre√ß√£o do Erro de Magnitude) ---
            # A produtividade de soja no Brasil √© ~3500 kg/ha.
            # Se o valor for > 10.000, provavelmente houve erro de casa decimal (ex: 30490).
            if float_val > 10000:
                float_val = float_val / 10.0
                
            return float_val
            
        except ValueError:
            return 0.0

    def process_file(self):
        logger.info(f"üìÇ Lendo arquivo: {self.file_path.name}")
        
        try:
            xls = pd.ExcelFile(self.file_path)
            
            # Busca aba de Produtividade
            target_sheet = None
            for sheet in xls.sheet_names:
                if "PRODUTIV" in sheet.upper() or "RENDIM" in sheet.upper():
                    target_sheet = sheet
                    break
            
            if not target_sheet:
                target_sheet = xls.sheet_names[0]

            logger.info(f"üìë Analisando aba: {target_sheet}")

            # Header Hunter
            df_raw = pd.read_excel(xls, sheet_name=target_sheet, header=None, nrows=20)
            header_idx = -1
            for i, row in df_raw.iterrows():
                row_str = " ".join([str(x) for x in row.values])
                if "REGI√ÉO/UF" in row_str and self.TARGET_COL_NAME in row_str:
                    header_idx = i
                    break
            
            if header_idx == -1:
                raise ValueError(f"Cabe√ßalho n√£o encontrado.")

            # Leitura dos dados
            df = pd.read_excel(xls, sheet_name=target_sheet, header=header_idx)
            df.columns = [str(c).strip() for c in df.columns]

            records = []
            
            for _, row in df.iterrows():
                uf_raw = str(row['REGI√ÉO/UF']).strip().upper()
                
                if uf_raw in self.TARGET_STATES:
                    val_raw = row[self.TARGET_COL_NAME]
                    yield_val = self._clean_number(val_raw)
                    
                    if yield_val > 0:
                        records.append({
                            "state_code": uf_raw,
                            "crop_year": "2023/2024",
                            "commodity": "soja", # Coluna que estava faltando no cache
                            "yield_kg_ha": yield_val,
                            "prev_yield_kg_ha": 0.0,
                            "source": "CONAB_FILE_OFFICIAL",
                            "source_ref": f"{self.file_path.name}",
                            "ingested_at": datetime.utcnow().isoformat()
                        })
                        logger.info(f"   -> Extra√≠do {uf_raw}: {yield_val} kg/ha")

            return records

        except Exception as e:
            logger.critical(f"Erro ao processar arquivo: {e}")
            return []

    async def run(self):
        if not self.file_path.exists():
            logger.error(f"Arquivo n√£o encontrado: {self.file_path}")
            return

        records = self.process_file()
        
        if records:
            try:
                self.db.client.table("official_crop_stats").upsert(
                    records, on_conflict="state_code, crop_year, commodity"
                ).execute()
                logger.info(f"‚úÖ Sucesso! {len(records)} registros corrigidos carregados.")
            except Exception as e:
                logger.error(f"Erro de persist√™ncia: {e}")
                logger.info("üí° DICA: Rode 'NOTIFY pgrst, 'reload schema';' no SQL Editor do Supabase.")
        else:
            logger.warning("Nenhum dado extra√≠do.")

if __name__ == "__main__":
    FILE_NAME = "data/raw/SojaSerieHist.xls"
    ingestor = ConabExactIngestor(FILE_NAME)
    asyncio.run(ingestor.run())


============================================================
ARQUIVO: seed_portfolio.py
LOCAL: scripts/seed_portfolio.py
============================================================

# scripts/seed_portfolio.py
import uuid
import random
import time
import logging
from core.db import DatabaseManager

# Configura√ß√£o de log simples
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def seed_database():
    """
    Popula o banco de dados com contratos fict√≠cios para testes de estresse.
    Cria dois clusters claros: 
    1. MT (Alto Risco: D√≠vida alta, Score baixo, Log√≠stica ruim)
    2. PR (Baixo Risco: D√≠vida baixa, Score alto, Log√≠stica boa)
    """
    random.seed(42)
    db = DatabaseManager(use_service_role=True)
    tag = "DEV_TEST_DATASET"  
    
    logger.info(f"üöÄ Iniciando Seed do Banco de Dados (TAG: {tag})...")
    
    try:
        # Limpeza de dados antigos para evitar duplicidade nos testes
        logger.info("üßπ Limpando dados antigos...")
        
        # Limpa resultados de backtest vinculados a essa tag
        sims = db.client.table("backtest_simulations").select("id").eq("simulation_name", tag).execute()
        for s in sims.data:
            db.client.table("backtest_results").delete().eq("simulation_id", s['id']).execute()
        
        # Limpa o portf√≥lio
        db.client.table("credit_portfolio").delete().eq("simulation_tag", tag).execute()
        time.sleep(1) # Breve espera para consist√™ncia do DB
        
    except Exception as e:
        logger.warning(f"Aviso durante limpeza (pode ser a primeira execu√ß√£o): {e}")

    contracts = []
    
    # --- CLUSTER 1: MATO GROSSO (HIGH RISK PROFILE) ---
    logger.info("üå± Gerando Cluster MT (Perfil de Alto Risco)...")
    mt_hubs = [
        {"name": "Sorriso", "lat": -12.54, "lon": -55.72},
        {"name": "Sinop", "lat": -11.86, "lon": -55.50},
        {"name": "Lucas", "lat": -13.07, "lon": -55.91}
    ]

    for i in range(50):
        hub = random.choice(mt_hubs)
        contracts.append({
            "id": str(uuid.uuid4()),
            "client_name": f"Produtor {hub['name']} {i+1:03d}",
            "latitude": hub['lat'] + random.uniform(-0.05, 0.05),
            "longitude": hub['lon'] + random.uniform(-0.05, 0.05),
            "state_code": "MT",
            "culture": "soja",
            "area_hectares": int(random.uniform(2000, 5000)),
            "estimated_yield_kg_ha": int(random.uniform(3200, 3500)),
            "loan_amount": round(random.uniform(10_000_000, 25_000_000), 2),
            "dist_to_port": int(random.uniform(2000, 2200)),
            # Perfil de Risco: Score Baixo + Alta Alavancagem
            "credit_score_serasa": random.randint(400, 600),
            "debt_to_income_ratio": random.uniform(0.75, 0.95),
            "payment_history_rating": "C",
            "simulation_tag": tag,
            "is_test_data": True
        })

    # --- CLUSTER 2: PARAN√Å (LOW RISK PROFILE) ---
    logger.info("üå± Gerando Cluster PR (Perfil de Baixo Risco)...")
    pr_hubs = [
        {"name": "Cascavel", "lat": -24.95, "lon": -53.45},
        {"name": "Toledo", "lat": -24.72, "lon": -53.74},
        {"name": "Londrina", "lat": -23.30, "lon": -51.16}
    ]

    for i in range(50):
        hub = random.choice(pr_hubs)
        contracts.append({
            "id": str(uuid.uuid4()),
            "client_name": f"Agro {hub['name']} {i+1:03d}",
            "latitude": hub['lat'] + random.uniform(-0.03, 0.03),
            "longitude": hub['lon'] + random.uniform(-0.03, 0.03),
            "state_code": "PR",
            "culture": "soja",
            "area_hectares": int(random.uniform(300, 800)),
            "estimated_yield_kg_ha": int(random.uniform(3800, 4200)),
            "loan_amount": round(random.uniform(1_000_000, 3_000_000), 2),
            "dist_to_port": int(random.uniform(400, 600)),
            # Perfil de Risco: Score Alto + Baixa Alavancagem
            "credit_score_serasa": random.randint(850, 980),
            "debt_to_income_ratio": random.uniform(0.10, 0.30),
            "payment_history_rating": "A",
            "simulation_tag": tag,
            "is_test_data": True
        })

    # Inser√ß√£o em Batch
    logger.info(f"üíæ Persistindo {len(contracts)} contratos no Supabase...")
    batch_size = 50
    for i in range(0, len(contracts), batch_size):
        batch = contracts[i:i + batch_size]
        db.client.table("credit_portfolio").insert(batch).execute()

    logger.info("‚úÖ Seed conclu√≠do com sucesso.")

if __name__ == "__main__":
    seed_database()


============================================================
ARQUIVO: project_txt.py
LOCAL: project_mapper/project_txt.py
============================================================

import os
from pathlib import Path

def transformar_projeto_definitivo(diretorio_raiz, arquivo_saida):
    # 1. Pastas que devem ser TOTALMENTE ignoradas (bibliotecas e lixo)
    pastas_proibidas = {
        'venv', '.venv', 'env', '__pycache__', '.git', 'node_modules',
        'site-packages', 'lib', 'include', 'bin', 'Lib', 'Scripts', 
        'dist', 'build', '.pytest_cache', '.idea', '.vscode', '.devcontainer', 
        '.devenv', 'devenv', '.direnv', '.nix'
    }
    
    # 2. Extens√µes que voc√™ quer manter
    extensoes_ok = {'.py', '.yml', '.yaml', '.sql', '.md', '.txt'}

    with open(arquivo_saida, 'w', encoding='utf-8') as f_out:
        for raiz, pastas, arquivos in os.walk(diretorio_raiz, topdown=True):
            path_raiz = Path(raiz)
            
            # FILTRO 1: Se qualquer parte do caminho atual estiver na lista proibida, pula
            if any(part in pastas_proibidas for part in path_raiz.parts):
                continue

            # FILTRO 2: Modifica 'pastas' para o os.walk n√£o entrar nelas
            pastas[:] = [p for p in pastas if p not in pastas_proibidas]

            for nome_arquivo in arquivos:
                # Pula o pr√≥prio script de sa√≠da para evitar loop infinito
                if nome_arquivo == arquivo_saida:
                    continue
                    
                ext = Path(nome_arquivo).suffix.lower()
                
                if ext in extensoes_ok:
                    caminho_completo = path_raiz / nome_arquivo
                    
                    # Filtro extra para garantir que nada de site-packages passe
                    if 'site-packages' in str(caminho_completo).lower():
                        continue

                    f_out.write(f"\n{'='*60}\n")
                    f_out.write(f"ARQUIVO: {nome_arquivo}\n")
                    f_out.write(f"LOCAL: {caminho_completo}\n")
                    f_out.write(f"{'='*60}\n\n")

                    try:
                        with open(caminho_completo, 'r', encoding='utf-8', errors='ignore') as f_in:
                            f_out.write(f_in.read())
                    except Exception as e:
                        f_out.write(f"--- Erro ao ler arquivo: {e} ---\n")
                    
                    f_out.write("\n\n")

# Para executar:
if __name__ == "__main__":
    # Garante que o arquivo de sa√≠da seja gerado na pasta atual
    transformar_projeto_definitivo('.', 'projeto_limpo_ia.txt')
    print("Sucesso! O arquivo 'projeto_limpo_ia.txt' foi gerado.")



============================================================
ARQUIVO: project_mapper.py
LOCAL: project_mapper/project_mapper.py
============================================================

import os
import ast

# --- Configura√ß√£o ---
OUTPUT_FILE = 'project_structure.txt'

# Lista de pastas e arquivos a serem ignorados
# Inclui sua lista personalizada e o pr√≥prio arquivo do script para evitar loop
IGNORE_LIST = {
    # Pastas de Ambientes e Bibliotecas
    'venv', '.venv', 'env', 'node_modules', 'site-packages', 
    'lib', 'include', 'bin', 'Lib', 'Scripts',
    
    # Git e Configura√ß√µes de IDE
    '.git', '.idea', '.vscode', 
    
    # Caches e Builds
    '__pycache__', '.pytest_cache', 'dist', 'build',
    
    # Ferramentas de Dev/Ambiente
    '.devcontainer', '.devenv', 'devenv', '.direnv', '.nix',
    
    # Arquivos espec√≠ficos (opcional)
    'project_mapper.py', '.DS_Store'
}

class StructureMapper(ast.NodeVisitor):
    def __init__(self):
        self.structure = []
        self.current_class = None

    def visit_ClassDef(self, node):
        self.current_class = node.name
        # Tenta pegar as bases da heran√ßa
        bases = [base.id for base in node.bases if isinstance(base, ast.Name)]
        self.structure.append(f"\n  [CLASSE] {node.name} (Herda de: {', '.join(bases)})")
        self.generic_visit(node)
        self.current_class = None

    def visit_FunctionDef(self, node):
        # Filtra 'self' dos argumentos para limpar a visualiza√ß√£o
        args = [arg.arg for arg in node.args.args if arg.arg != 'self']
        prefix = "    " if self.current_class else ""
        type_def = "[M√âTODO]" if self.current_class else "[FUN√á√ÉO]"
        self.structure.append(f"{prefix}  {type_def} {node.name}(args: {', '.join(args)})")
        self.generic_visit(node)

    def visit_Assign(self, node):
        for target in node.targets:
            # Captura self.variavel (Atributos de classe definidos no __init__ ou m√©todos)
            if isinstance(target, ast.Attribute) and isinstance(target.value, ast.Name) and target.value.id == 'self':
                self.structure.append(f"      -> Atributo: self.{target.attr}")
            # Captura Constantes Globais (UPPERCASE)
            elif isinstance(target, ast.Name) and target.id.isupper():
                self.structure.append(f"  [CONSTANTE] {target.id}")

def generate_map():
    print(f"Iniciando mapeamento... Ignorando: {len(IGNORE_LIST)} itens configurados.")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write("MAPA DE ESTRUTURA E VARI√ÅVEIS DO PROJETO\n")
        f.write("Use este guia para entender a arquitetura sem ler todo o c√≥digo.\n")
        
        for root, dirs, files in os.walk('.'):
            # Modifica a lista 'dirs' in-place para que o os.walk n√£o entre nas pastas ignoradas
            # Isso melhora muito a performance evitando ler node_modules ou venv
            dirs[:] = [d for d in dirs if d not in IGNORE_LIST]
            
            for file in files:
                # Pula se o arquivo estiver na lista de ignorados
                if file in IGNORE_LIST:
                    continue
                
                # Processa apenas arquivos Python
                if file.endswith('.py'):
                    path = os.path.join(root, file)
                    # Remove o './' inicial para ficar mais limpo
                    clean_path = path.replace('.\\', '').replace('./', '')
                    
                    f.write(f"\n{'='*50}\nARQUIVO: {clean_path}\n{'='*50}")
                    
                    try:
                        with open(path, 'r', encoding='utf-8') as source:
                            content = source.read()
                            if not content.strip():
                                f.write("\n  [ARQUIVO VAZIO]")
                                continue
                                
                            tree = ast.parse(content)
                            mapper = StructureMapper()
                            mapper.visit(tree)
                            
                            if not mapper.structure:
                                f.write("\n  [SEM CLASSES/FUN√á√ïES DETECTADAS]")
                            else:
                                for line in mapper.structure:
                                    f.write(f"{line}\n")
                                    
                    except SyntaxError:
                        f.write(f"\n  [ERRO DE SINTAXE] O arquivo n√£o p√¥de ser lido pelo AST.\n")
                    except Exception as e:
                        f.write(f"\n  [ERRO AO LER] {e}\n")

    print(f"Mapa gerado com sucesso em: {os.path.abspath(OUTPUT_FILE)}")

if __name__ == "__main__":
    generate_map()


============================================================
ARQUIVO: worker_market.py
LOCAL: data_ingestion/worker_market.py
============================================================

# worker_market.py
from core.market_router import MarketRouter # <--- Novo componente
from core.db import DatabaseManager
from core.env import load_config
from core.logger import get_logger

logger = get_logger("WorkerMarket")

def fetch_and_save():
    config = load_config()
    db = DatabaseManager(use_service_role=True)
    router = MarketRouter(config) # Inje√ß√£o de Depend√™ncia via Config
    
    tickers = config.get('tickers', [])
    
    logger.info("üöÄ Iniciando Ingest√£o H√≠brida (Padr√£o Institucional)...")
    
    # O Router cuida da complexidade de APIs
    df_unified = router.fetch_batch(tickers)
    
    if df_unified.empty:
        logger.error("‚ùå Nenhum dado coletado de nenhuma fonte.")
        return

    # Transforma√ß√£o para formato do Banco
    records = []
    for index, row in df_unified.iterrows():
        records.append({
            "ticker": row['ticker'],
            "date": index.strftime('%Y-%m-%d'),
            "open": float(row['Open']),
            "high": float(row['High']),
            "low": float(row['Low']),
            "close": float(row['Close']),
            "volume": float(row['Volume']),
            "source": row.get('source', 'UNKNOWN') # <--- CAMPO NOVO DE AUDITORIA
        })

    # Persist√™ncia
    if records:
        logger.info(f"üíæ Salvando {len(records)} registros com rastreabilidade de fonte.")
        db.client.table("market_prices").upsert(records, on_conflict="ticker, date").execute()

if __name__ == "__main__":
    fetch_and_save()

